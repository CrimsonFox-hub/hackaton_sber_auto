{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc768e75-186e-4514-8634-0987750111b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV  # Для закомментированного GridSearchCV\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    KBinsDiscretizer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf99e7d-c828-4181-9f02-aeecdece6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Определение вспомогательных функций\n",
    "\n",
    "\n",
    "def process_notset_none(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Обнаруживает строки 'not set' и 'none' и заменяет соответствующие им значения на np.nan во всех столбцах DataFrame.\n",
    "    Модифицирует DataFrame напрямую.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"Ошибка: Входной аргумент должен быть pandas DataFrame.\")\n",
    "        return\n",
    "\n",
    "    strings_to_find_and_replace = [\"not set\", \"none\"]\n",
    "\n",
    "    print(\"Начало обработки 'not set' и 'none'\", file=sys.stderr)\n",
    "    print(\n",
    "        \"--------------------------------------------------------------\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_series_str = df[col].astype(str)\n",
    "        mask_not_set = col_series_str.str.contains(\n",
    "            strings_to_find_and_replace[0], case=False, regex=False\n",
    "        )\n",
    "        mask_none = col_series_str.str.contains(\n",
    "            strings_to_find_and_replace[1], case=False, regex=False\n",
    "        )\n",
    "        combined_mask = mask_not_set | mask_none\n",
    "        total_found = combined_mask.sum()\n",
    "\n",
    "        if total_found > 0:\n",
    "            # print(f\"\\nСтолбец '{col}': обнаружено {total_found} вхождений\", file=sys.stderr)\n",
    "            n_nan_before = df[col].isnull().sum()\n",
    "            # print(f\"  -> Количество NaN до замены: {n_nan_before}\", file=sys.stderr)\n",
    "            df.loc[combined_mask, col] = np.nan\n",
    "            # final_count_nan = df[col].isnull().sum()\n",
    "            # print(f\"  -> После замены, количество NaN в столбце: {final_count_nan}\", file=sys.stderr)\n",
    "\n",
    "    print(\"\\nОбработка 'not set' и 'none' завершена.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"-------------------------------------------------------------\", file=sys.stderr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8166ba2-fdb8-4852-b132-5a56b93bd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_column_with_nulls(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Кодирует значения в указанном столбце числовыми метками (начиная с 1),\n",
    "    заменяя пропуски (NaN) на 0. Добавляет новый столбец {column_name}_encoded.\n",
    "    Возвращает новый DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # Работаем с копией, как в оригинале\n",
    "    if column_name not in df.columns:\n",
    "        print(\n",
    "            f\"Ошибка: Столбец '{column_name}' не найден в DataFrame.\", file=sys.stderr\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    # .astype(str) для избежания проблем с типами в factorize\n",
    "    encoded, uniques = pd.factorize(df[column_name].astype(str))\n",
    "    # Заменяем -1 (метка для NaN в factorize) на 0, остальные метки увеличиваем на 1\n",
    "    df[f\"{column_name}_encoded\"] = np.where(encoded == -1, 0, encoded + 1)\n",
    "    df[f\"{column_name}_encoded\"] = df[f\"{column_name}_encoded\"].astype(\"int64\")\n",
    "\n",
    "    # print(f\"Кодирование столбца '{column_name}':\", file=sys.stderr)\n",
    "    # print(f\"Уникальных значений (вкл. NaN): {len(uniques) + (1 if -1 in encoded else 0)}\", file=sys.stderr)\n",
    "    # print(f\"Максимальный код: {df[f'{column_name}_encoded'].max()}\", file=sys.stderr)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea64cd95-4b02-4a6e-971c-490b7debb02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode(x):\n",
    "    \"\"\"Вспомогательная функция для получения моды, обрабатывающая пустые Series\"\"\"\n",
    "    mode_val = x.mode()\n",
    "    if not mode_val.empty:\n",
    "        return mode_val[0]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be485a15-5918-4db0-afe7-8fcd0b305912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_by_groups(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Заполняет пропущенные значения в DataFrame по группам и общей модой.\n",
    "    Работает in-place для экономии памяти.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"Начало оптимизированного заполнения пропусков по группам и модой (in-place).\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\n",
    "        \"--------------------------------------------------------------\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # Связка 1: utm_medium + device_category -> utm_source\n",
    "    if \"utm_source\" in df.columns and df[\"utm_source\"].isna().any():\n",
    "        print(\n",
    "            \"Заполнение пропусков в 'utm_source' по группам ('utm_medium', 'device_category').\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        if \"utm_medium\" in df.columns and \"device_category\" in df.columns:\n",
    "            group_modes_source = df.groupby([\"utm_medium\", \"device_category\"])[\n",
    "                \"utm_source\"\n",
    "            ].transform(get_mode)\n",
    "            df[\"utm_source\"].fillna(group_modes_source, inplace=True)\n",
    "        print(\n",
    "            f\"Пропусков в 'utm_source' после заполнения по группе: {df['utm_source'].isna().sum()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # Связка 2: utm_medium + device_os -> utm_campaign\n",
    "    if \"utm_campaign\" in df.columns and df[\"utm_campaign\"].isna().any():\n",
    "        print(\n",
    "            \"Заполнение пропусков в 'utm_campaign' по группам ('utm_medium', 'device_os').\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        if \"utm_medium\" in df.columns and \"device_os\" in df.columns:\n",
    "            group_modes_campaign = df.groupby([\"utm_medium\", \"device_os\"])[\n",
    "                \"utm_campaign\"\n",
    "            ].transform(get_mode)\n",
    "            df[\"utm_campaign\"].fillna(group_modes_campaign, inplace=True)\n",
    "        print(\n",
    "            f\"Пропусков в 'utm_campaign' после заполнения по группе: {df['utm_campaign'].isna().sum()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # Заполнение оставшихся пропусков в более широких группах\n",
    "    group_columns_broad = [\"utm_medium\", \"device_category\", \"device_os\"]\n",
    "    # Проверяем наличие всех столбцов для группировки\n",
    "    if all(col in df.columns for col in group_columns_broad):\n",
    "        print(\n",
    "            f\"Заполнение оставшихся пропусков по более широким группам {group_columns_broad} (для столбцов с пропусками).\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        cols_with_nan_after_specific = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "        for col in cols_with_nan_after_specific:\n",
    "            # Пропускаем, если уже нет пропусков после специфичного заполнения\n",
    "            if col in [\"utm_source\", \"utm_campaign\"] and not df[col].isna().any():\n",
    "                continue\n",
    "\n",
    "            # print(f\"  Обработка столбца '{col}'...\", file=sys.stderr)\n",
    "            # Вычисляем моду для текущего столбца в широких группах\n",
    "            # Игнорируем пропуски в group_columns_broad при группировке по умолчанию\n",
    "            group_modes_broad_col = df.groupby(group_columns_broad)[col].transform(\n",
    "                get_mode\n",
    "            )\n",
    "\n",
    "            # Заполняем пропуски\n",
    "            df[col].fillna(group_modes_broad_col, inplace=True)\n",
    "            # print(f\"    Пропусков в '{col}' после заполнения по широкой группе: {df[col].isna().sum()}\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Не все столбцы для широкой группировки ({group_columns_broad}) найдены. Пропуск шага.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # Финальное заполнение модой для любых оставшихся пропусков\n",
    "    print(\n",
    "        \"Финальное заполнение любых оставшихся пропусков общей модой по столбцам.\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    cols_with_nan_final = df.columns[df.isna().any()].tolist()\n",
    "    for col in cols_with_nan_final:\n",
    "        mode_val = df[col].mode()\n",
    "        if not mode_val.empty:\n",
    "            # print(f\"  Финальное заполнение '{col}' общей модой ({mode_val[0]}).\", file=sys.stderr)\n",
    "            df[col].fillna(mode_val[0], inplace=True)\n",
    "        else:\n",
    "            print(\n",
    "                f\"  Нет не-NaN значений для вычисления общей моды в '{col}', пропуски остаются.\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        \"\\nОптимизированное заполнение пропусков завершено (in-place).\", file=sys.stderr\n",
    "    )\n",
    "    print(\n",
    "        \"-------------------------------------------------------------\", file=sys.stderr\n",
    "    )\n",
    "\n",
    "    return df  # Возвращаем для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8143a91-6b8f-4b8a-b420-3eb7863ee487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_hits_and_events(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Заполняет пропуски в столбцах hit_referer и event_label\n",
    "    оптимизированным способом, используя группировку и константы.\n",
    "    Работает НАПРЯМУЮ с входным DataFrame (in-place).\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"Начало оптимизированного заполнения пропусков в hit_referer и event_label (in-place).\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\n",
    "        \"--------------------------------------------------------------\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # Заполнение hit_referer первым не-NaN значением в группе по hit_page_path\n",
    "    if (\n",
    "        \"hit_referer\" in df.columns\n",
    "        and \"hit_page_path\" in df.columns\n",
    "        and df[\"hit_referer\"].isna().any()\n",
    "    ):\n",
    "        print(\n",
    "            \"Заполнение пропусков в 'hit_referer' первым значением в группе по 'hit_page_path'.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        referer_by_page = df.groupby(\"hit_page_path\")[\"hit_referer\"].transform(\"first\")\n",
    "        df[\"hit_referer\"].fillna(referer_by_page, inplace=True)\n",
    "        print(\n",
    "            f\"Пропусков в 'hit_referer' после заполнения по группе: {df['hit_referer'].isna().sum()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "    elif \"hit_referer\" in df.columns:\n",
    "        print(\n",
    "            \"'hit_referer' не требует заполнения по группе или 'hit_page_path' отсутствует.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # Заполнение event_label первым не-NaN значением в группе по event_action\n",
    "    if (\n",
    "        \"event_label\" in df.columns\n",
    "        and \"event_action\" in df.columns\n",
    "        and df[\"event_label\"].isna().any()\n",
    "    ):\n",
    "        print(\n",
    "            \"Заполнение пропусков в 'event_label' первым значением в группе по 'event_action'.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        label_by_action = df.groupby(\"event_action\")[\"event_label\"].transform(\"first\")\n",
    "        df[\"event_label\"].fillna(label_by_action, inplace=True)\n",
    "        print(\n",
    "            f\"Пропусков в 'event_label' после заполнения по группе: {df['event_label'].isna().sum()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "    elif \"event_label\" in df.columns:\n",
    "        print(\n",
    "            \"'event_label' не требует заполнения по группе или 'event_action' отсутствует.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # --- Финальное заполнение оставшихся пропусков ---\n",
    "    if \"hit_referer\" in df.columns and df[\"hit_referer\"].isna().any():\n",
    "        print(\n",
    "            \"Финальное заполнение оставшихся пропусков в 'hit_referer' значением 'direct'.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        df[\"hit_referer\"].fillna(\"direct\", inplace=True)\n",
    "        print(\n",
    "            f\"Пропусков в 'hit_referer' после финального заполнения: {df['hit_referer'].isna().sum()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    if \"event_label\" in df.columns and df[\"event_label\"].isna().any():\n",
    "        print(\n",
    "            \"Финальное заполнение оставшихся пропусков в 'event_label' значением 'none'.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        df[\"event_label\"].fillna(\"none\", inplace=True)\n",
    "        print(\n",
    "            f\"Пропусков в 'event_label' после финального заполнения: {df['event_label'].isna().sum()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"\\nОптимизированное заполнение пропусков в hit_referer и event_label завершено (in-place).\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\n",
    "        \"-------------------------------------------------------------\", file=sys.stderr\n",
    "    )\n",
    "\n",
    "    return df  # Возвращаем для удобства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e8115a-64aa-48ca-8d6e-f623b8407d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_session_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Агрегирует данные хитов до уровня сессий, извлекая ключевые метрики.\n",
    "    \"\"\"\n",
    "    print(\"Начало агрегации данных хитов по сессиям.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"--------------------------------------------------------------\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    agg_rules = {\n",
    "        \"hit_number\": [\"min\", \"max\", \"count\"],\n",
    "        # Проверяем существование колонок перед добавлением правила\n",
    "        **(\n",
    "            {\"hit_page_path\": lambda x: x.iloc[0] if not x.empty else np.nan}\n",
    "            if \"hit_page_path\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "        **(\n",
    "            {\n",
    "                \"event_category_grouped\": lambda x: (\n",
    "                    x.value_counts().index[0] if not x.empty else np.nan\n",
    "                )\n",
    "            }\n",
    "            if \"event_category_grouped\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "        **(\n",
    "            {\n",
    "                \"event_action_grouped\": lambda x: (\n",
    "                    x.value_counts().index[0] if not x.empty else np.nan\n",
    "                )\n",
    "            }\n",
    "            if \"event_action_grouped\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "        **({\"hit_time_2\": \"sum\"} if \"hit_time_2\" in df.columns else {}),\n",
    "        **(\n",
    "            {\n",
    "                \"hit_referer_encoded\": lambda x: (\n",
    "                    x.value_counts().index[0] if not x.empty else np.nan\n",
    "                )\n",
    "            }\n",
    "            if \"hit_referer_encoded\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "        **(\n",
    "            {\n",
    "                \"event_label_encoded\": lambda x: (\n",
    "                    x.value_counts().index[0] if not x.empty else np.nan\n",
    "                )\n",
    "            }\n",
    "            if \"event_label_encoded\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Добавляем все UTM-метки, если они есть в данных и не были удалены ранее\n",
    "    utm_columns_in_hits = [\n",
    "        col for col in df.columns if col.startswith(\"utm_\") and col not in agg_rules\n",
    "    ]\n",
    "    for col in utm_columns_in_hits:\n",
    "        agg_rules[col] = lambda x: (\n",
    "            x.iloc[0] if not x.empty else np.nan\n",
    "        )  # берем первое значение\n",
    "\n",
    "    # Убедимся, что ключи в agg_rules соответствуют столбцам в df\n",
    "    # (кроме session_id, по которому идет группировка)\n",
    "    # Этот check уже встроен при использовании **{} выражений выше\n",
    "\n",
    "    # Проверяем, есть ли 'session_id' для группировки\n",
    "    if \"session_id\" not in df.columns:\n",
    "        print(\n",
    "            \"Ошибка: Столбец 'session_id' отсутствует в DataFrame для агрегации.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return pd.DataFrame()  # Возвращаем пустой DF при ошибке\n",
    "\n",
    "    try:\n",
    "        aggregated = df.groupby(\"session_id\").agg(agg_rules)\n",
    "        print(\"Агрегация выполнена.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при выполнении агрегации: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Строим явный словарь переименования из MultiIndex/SingleIndex имен в финальные имена\n",
    "\n",
    "    rename_map = {}\n",
    "    for original_col_tuple in aggregated.columns.values:\n",
    "        # Определяем имя столбца после агрегации (MultiIndex tuple или SingleIndex string)\n",
    "        if isinstance(original_col_tuple, tuple):\n",
    "            # Для MultiIndex: ('original_name', 'agg_func') или ('original_name', '<lambda>')\n",
    "            original_name = original_col_tuple[0]\n",
    "            agg_name = original_col_tuple[1]\n",
    "            col_key = original_col_tuple  # Ключ в aggregated.columns\n",
    "        else:\n",
    "            # Для SingleIndex (редко после agg, но может быть после reset_index)\n",
    "            original_name = original_col_tuple\n",
    "            agg_name = None\n",
    "            col_key = original_col_tuple  # Ключ в aggregated.columns\n",
    "\n",
    "        # Определяем финальное имя\n",
    "        final_name = str(original_col_tuple)  # По умолчанию\n",
    "\n",
    "        if isinstance(original_col_tuple, tuple):\n",
    "            if original_name == \"hit_number\":\n",
    "                if agg_name == \"min\":\n",
    "                    final_name = \"first_hit_number\"\n",
    "                elif agg_name == \"max\":\n",
    "                    final_name = \"last_hit_number\"\n",
    "                elif agg_name == \"count\":\n",
    "                    final_name = \"total_hits\"\n",
    "            elif original_name == \"hit_time_2\" and agg_name == \"sum\":\n",
    "                final_name = \"total_time\"\n",
    "            elif agg_name == \"<lambda>\":\n",
    "                if original_name == \"hit_page_path\":\n",
    "                    final_name = \"entry_page\"\n",
    "                elif original_name == \"event_category_grouped\":\n",
    "                    final_name = \"main_category_grouped\"\n",
    "                elif original_name == \"event_action_grouped\":\n",
    "                    final_name = \"main_action_grouped\"\n",
    "                elif original_name == \"hit_referer_encoded\":\n",
    "                    final_name = \"main_referer\"\n",
    "                elif original_name == \"event_label_encoded\":\n",
    "                    final_name = \"main_label\"\n",
    "                elif original_name in utm_columns_in_hits:\n",
    "                    final_name = original_name  # У UTM-меток сохраняем исходное имя\n",
    "                else:\n",
    "                    final_name = f\"{original_name}_lambda_agg\"  # Fallback\n",
    "            else:\n",
    "                # Для других стандартных агрегаций (mean, max, etc.), если они были бы добавлены\n",
    "                final_name = f\"{original_name}_{agg_name}\"\n",
    "\n",
    "        # Добавляем в словарь переименований\n",
    "        rename_map[col_key] = final_name\n",
    "\n",
    "    # Применяем переименование\n",
    "    # Преобразуем MultiIndex в Index, применяя переименования\n",
    "    try:\n",
    "        aggregated.columns = [\n",
    "            rename_map.get(col, str(col)) for col in aggregated.columns\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при применении переименования столбцов: {e}\", file=sys.stderr)\n",
    "        aggregated.columns = [\n",
    "            \"_\".join(col).strip(\"_\") if isinstance(col, tuple) else str(col)\n",
    "            for col in aggregated.columns.values\n",
    "        ]\n",
    "\n",
    "    # Сбрасываем индекс, чтобы session_id стал обычным столбцом\n",
    "    # session_id до этого момента был индексом\n",
    "    aggregated = aggregated.reset_index()\n",
    "    # После reset_index, столбец с session_id называется 'session_id'\n",
    "\n",
    "    print(\"Переименование и финальная обработка столбцов завершена.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"-------------------------------------------------------------\", file=sys.stderr\n",
    "    )\n",
    "\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91c804a-5717-42df-9c79-e25433ce067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Она используется и для EDA, и для анализа, может быть удалена в продакшн-скрипте.\n",
    "def analyze_dataframe_columns(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Выполняет анализ каждого столбца pandas DataFrame, выводя информацию\n",
    "    об уникальных значениях, первых/последних/частых значениях,\n",
    "    количестве и проценте пропусков, а также типе данных.\n",
    "    Используется для EDA.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"Ошибка: Входной аргумент должен быть pandas DataFrame.\")\n",
    "        return\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"DataFrame пуст.\")\n",
    "        return\n",
    "\n",
    "    for col in df.columns:\n",
    "        print(f\"\\n--- Столбец: {col} ---\")\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "        nunique = df[col].nunique()\n",
    "        print(f\"Количество уникальных значений: {nunique}\")\n",
    "\n",
    "        print(\"Первые 5 значений:\")\n",
    "        if not df[col].empty:\n",
    "            try:\n",
    "                # Добавляем .astype(str) на всякий случай для сложных объектов\n",
    "                print(df[col].head().astype(str).tolist())\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Не удалось вывести первые 5 значений (ошибка: {e}). Показываем первые 5 строк серии:\\n{df[col].head()}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"Столбец пуст.\")\n",
    "\n",
    "        print(\"\\nТоп-5 наиболее частых значений:\")\n",
    "        if not df[col].empty and df[col].count() > 0:\n",
    "            print(df[col].value_counts().head())\n",
    "        else:\n",
    "            print(\"Столбец пуст или содержит только пропуски.\")\n",
    "\n",
    "        print(\"\\nТоп-5 наиболее редких значений:\")\n",
    "        if not df[col].empty and df[col].count() > 0:\n",
    "            value_counts = df[col].value_counts()\n",
    "            if len(value_counts) >= 5:\n",
    "                print(value_counts.tail())\n",
    "            else:\n",
    "                # Если уникальных значений меньше 5, показываем все\n",
    "                print(value_counts)\n",
    "        else:\n",
    "            print(\"Столбец пуст или содержит только пропуски.\")\n",
    "\n",
    "        n_missing = df[col].isnull().sum()\n",
    "        print(f\"\\nКоличество пропусков: {n_missing}\")\n",
    "\n",
    "        total_count = len(df[col])\n",
    "        if total_count > 0:\n",
    "            percent_missing = (n_missing / total_count) * 100\n",
    "            print(f\"Процент пропусков: {percent_missing:.2f}%\")\n",
    "        else:\n",
    "            print(\"Процент пропусков: 0.00% (столбец пуст)\")\n",
    "\n",
    "        data_type = df[col].dtype\n",
    "        print(f\"Тип данных: {data_type}\")\n",
    "        print(\"----------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0542cce2-0989-41fa-b7a4-673cf3cadba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_save(\n",
    "    sessions_df: pd.DataFrame,\n",
    "    hits_df: pd.DataFrame,\n",
    "    output_pkl: str = \"merged_data.pkl\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Объединяет данные сессий и хитов, сохраняет результат в PKL и возвращает датафрейм\n",
    "    \"\"\"\n",
    "\n",
    "    # Вложенная функция для оптимизации типов данных\n",
    "    def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print(\"  Оптимизация типов данных...\", file=sys.stderr)\n",
    "        initial_memory = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "\n",
    "        for col in df.columns:\n",
    "            # Оптимизация объектов (строк)\n",
    "            if df[col].dtype == \"object\":\n",
    "                # Если уникальных значений меньше порога, преобразуем в категорию\n",
    "                # Порог 0.5 может быть агрессивным, 0.05-0.1 обычно безопаснее\n",
    "                if df[col].nunique() / len(df) < 0.1:  # Изменил порог для безопасности\n",
    "                    try:\n",
    "                        df[col] = df[col].astype(\"category\")\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"  Не удалось преобразовать столбец '{col}' в категорию: {e}\",\n",
    "                            file=sys.stderr,\n",
    "                        )\n",
    "\n",
    "            # Оптимизация чисел\n",
    "            elif pd.api.types.is_integer_dtype(df[col]):\n",
    "                # Попытка downcast для целых\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"  Не удалось downcast столбец '{col}' (integer): {e}\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "\n",
    "            elif pd.api.types.is_float_dtype(df[col]):\n",
    "                # Попытка downcast для float\n",
    "                try:\n",
    "                    df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"  Не удалось downcast столбец '{col}' (float): {e}\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "        final_memory = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "        print(\n",
    "            f\"  Память после оптимизации: {final_memory:.2f} MB (было {initial_memory:.2f} MB)\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    print(\"Начало объединения данных.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"--------------------------------------------------------------\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # 1. Оптимизация типов данных перед объединением\n",
    "\n",
    "    print(\"Оптимизация типов данных перед объединением...\", file=sys.stderr)\n",
    "    sessions_df = optimize_dtypes(sessions_df)\n",
    "    hits_df = optimize_dtypes(hits_df)\n",
    "\n",
    "    # 2. Объединение данных\n",
    "    print(\"Объединение данных по session_id...\", file=sys.stderr)\n",
    "    # Проверим, существует ли 'session_id' в обоих DF (необходимо будет для валидации)\n",
    "    if \"session_id\" not in sessions_df.columns:\n",
    "        print(\n",
    "            \"Ошибка: Столбец 'session_id' отсутствует в sessions_df.\", file=sys.stderr\n",
    "        )\n",
    "        # Возвращаем один из DF или пустой DF, в зависимости от желаемого поведения\n",
    "        return pd.DataFrame()\n",
    "    if \"session_id\" not in hits_df.columns:\n",
    "        print(\"Ошибка: Столбец 'session_id' отсутствует в hits_df.\", file=sys.stderr)\n",
    "        # Если sessions_df есть, можно вернуть его, или пустой DF\n",
    "        return sessions_df.copy()  # Вернем sessions_df, т.к. это left merge\n",
    "\n",
    "    try:\n",
    "        merged_df = pd.merge(\n",
    "            sessions_df,\n",
    "            hits_df,\n",
    "            on=\"session_id\",\n",
    "            how=\"left\",  # Сохраняем все сессии, даже если нет соответствующих хитов\n",
    "            # validate='one_to_many' # Может быть медленно на больших данных, опционально\n",
    "        )\n",
    "        print(\"Объединение завершено.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при объединении данных: {e}\", file=sys.stderr)\n",
    "        # Возвращаем пустой DataFrame или обрабатываем ошибку\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 3. Оптимизация объединенных данных\n",
    "    print(\"Оптимизация объединенных данных...\", file=sys.stderr)\n",
    "    merged_df = optimize_dtypes(merged_df)\n",
    "\n",
    "    # 4. Сохранение в PKL\n",
    "    print(f\"Сохранение данных в {output_pkl}...\", file=sys.stderr)\n",
    "    try:\n",
    "        with open(output_pkl, \"wb\") as f:\n",
    "            pickle.dump(merged_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # 5. Проверка размера файла\n",
    "        pkl_size = Path(output_pkl).stat().st_size / (1024 * 1024)\n",
    "        print(f\"Файл сохранен. Размер: {pkl_size:.2f} MB\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении файла {output_pkl}: {e}\", file=sys.stderr)\n",
    "\n",
    "    print(\"Объединение и сохранение завершены.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"-------------------------------------------------------------\", file=sys.stderr\n",
    "    )\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb145f0-d37a-45ad-9b9e-2a8191fed6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cardinality(\n",
    "    df: pd.DataFrame, categorical_features: list, max_categories: int = 50\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Уменьшает количество категорий для высококардинальных признаков\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in categorical_features:\n",
    "        if (\n",
    "            col in df.columns and df[col].dtype == \"object\"\n",
    "        ):  # Применяем только к object типам, т.к. категории обрабатываются OrdinalEncoder\n",
    "            if df[col].nunique() > max_categories:\n",
    "                # print(f\"Уменьшение кардинальности для '{col}' ({df[col].nunique()} > {max_categories})\", file=sys.stderr)\n",
    "                top_categories = (\n",
    "                    df[col].value_counts().nlargest(max_categories - 1).index\n",
    "                )\n",
    "                # Убедимся, что 'OTHER' не входит в топ категории, если уже существует\n",
    "                if \"OTHER\" in top_categories:\n",
    "                    top_categories = (\n",
    "                        df[col].value_counts().nlargest(max_categories).index\n",
    "                    )  # Берем на одну больше, если OTHER в топе\n",
    "                    if (\n",
    "                        \"OTHER\" in top_categories\n",
    "                    ):  # Если OTHER все еще в топе после взятия больше, удаляем его\n",
    "                        top_categories = top_categories.drop(\"OTHER\")\n",
    "                        top_categories = top_categories[\n",
    "                            : max_categories - 1\n",
    "                        ]  # Обрезаем до нужного размера\n",
    "                df[col] = np.where(df[col].isin(top_categories), df[col], \"OTHER\")\n",
    "                # После замены на 'OTHER' можем преобразовать обратно в category для экономии памяти\n",
    "                try:\n",
    "                    df[col] = df[col].astype(\"category\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Не удалось преобразовать столбец '{col}' в категорию после reduce_cardinality: {e}\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "            elif df[col].nunique() > 0:\n",
    "                # Если не высококардинальный object, конвертируем в category\n",
    "                try:\n",
    "                    df[col] = df[col].astype(\"category\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Не удалось преобразовать столбец '{col}' в категорию: {e}\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb97507e-dc3a-4055-a13c-e7a061c5cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rare_classes(y: pd.Series, min_samples: int = 2) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Возвращает индекс классов целевой переменной,\n",
    "    количество образцов в которых >= min_samples.\n",
    "    \"\"\"\n",
    "    if not isinstance(y, pd.Series) or y.empty:\n",
    "        print(\n",
    "            \"Предупреждение: y для filter_rare_classes пусто или не Series.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return pd.Index([])  # Возвращаем пустой индекс для пустого ввода\n",
    "\n",
    "    value_counts = y.value_counts()\n",
    "    # Исключаем редкие классы, включая те, которые могут быть NaN (если target не обрабатывался)\n",
    "    # Хотя обычно target_column не должен содержать NaN\n",
    "    valid_classes = value_counts[value_counts >= min_samples].index\n",
    "    # Убедимся, что NaN не попадает в valid_classes, если он есть в value_counts\n",
    "    if pd.isna(valid_classes).any():\n",
    "        valid_classes = valid_classes.dropna()\n",
    "\n",
    "    if len(valid_classes) < len(value_counts):\n",
    "        rare_count = len(value_counts) - len(valid_classes)\n",
    "        print(\n",
    "            f\"Отфильтровано {rare_count} редких классов (менее {min_samples} образцов).\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        # print(f\"Оставшиеся классы: {valid_classes.tolist()}\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\"Редкие классы не найдены (все классы >= min_samples).\", file=sys.stderr)\n",
    "\n",
    "    return valid_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54998ecb-e13a-492c-a5c0-2f5f8aa538b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_lgbm_model(\n",
    "    data: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    output_file: str = \"model.pkl\",\n",
    "    min_class_samples: int = 5,\n",
    "):\n",
    "    \"\"\"Обучает LGBM модель с оптимизацией памяти и сохраняет в файл\"\"\"\n",
    "    print(\n",
    "        \"\\n==============================================================================\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\"Начало обучения LGBM модели.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"--------------------------------------------------------------\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # === ОТЛАДОЧНЫЙ ПРИНТ: Проверка наличия целевого столбца в начале функции ===\n",
    "    print(\n",
    "        f\"Debug: Проверка наличия целевого столбца '{target_column}' в data.columns в начале train_and_save_lgbm_model...\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\n",
    "        f\"Debug: '{target_column}' in data.columns: {target_column in data.columns}\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    if target_column not in data.columns:\n",
    "        print(\n",
    "            f\"Ошибка: Целевой столбец '{target_column}' не найден в DataFrame.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return None\n",
    "    # ===========================================================================\n",
    "\n",
    "    # Применяем уменьшение кардинальности ДО разделения на train/test\n",
    "    # Это важно, чтобы 'OTHER' категория была согласована между выборками\n",
    "    # Функция reduce_cardinality обрабатывает копию, что безопасно.\n",
    "    data_processed = reduce_cardinality(data, categorical_features)\n",
    "\n",
    "    # Разделяем на признаки и целевую переменную\n",
    "    # Используем data_processed после reduce_cardinality\n",
    "    X = data_processed.drop(columns=[target_column, \"session_id\"], errors=\"ignore\")\n",
    "    y = data_processed[target_column]\n",
    "\n",
    "    # Проверяем целевую переменную перед фильтрацией\n",
    "    if y.isnull().any():\n",
    "        print(\n",
    "            f\"Предупреждение: Целевая переменная '{target_column}' содержит пропуски ({y.isnull().sum()}). Они будут исключены при фильтрации.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # Фильтруем редкие классы и пропуски в целевой переменной\n",
    "    valid_classes = filter_rare_classes(y, min_samples=min_class_samples)\n",
    "    mask = y.isin(valid_classes)  # Маска для оставшихся валидных классов\n",
    "    X_filtered = X[mask].copy()\n",
    "    y_filtered = y[mask].copy()\n",
    "\n",
    "    print(\n",
    "        f\"Обучение будет проходить на {len(y_filtered)} образцах после фильтрации.\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # Освобождаем память\n",
    "    del (\n",
    "        X,\n",
    "        y,\n",
    "        mask,\n",
    "        data_processed,\n",
    "    )  # Удаляем data_processed, т.к. используем X_filtered, y_filtered\n",
    "    gc.collect()\n",
    "\n",
    "    if X_filtered.empty or y_filtered.empty:\n",
    "        print(\n",
    "            \"Ошибка: Не осталось данных для обучения после фильтрации редких классов или пропусков.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Разделяем на train/test\n",
    "    # Убедимся, что стратификация возможна (т.е. в каждом классе >= 2 образцов в train/test)\n",
    "    # filter_rare_classes с min_samples=5 уже помогает.\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_filtered, y_filtered, test_size=0.2, random_state=42, stratify=y_filtered\n",
    "        )\n",
    "        print(\n",
    "            f\"Данные разделены на train ({len(X_train)}) и test ({len(X_test)}).\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\n",
    "            f\"Ошибка при разделении данных на train/test со стратификацией: {e}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        print(\n",
    "            \"Возможно, в каком-то классе осталось менее 2 образцов после фильтрации.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Создаем пайплайны предобработки\n",
    "    numeric_transformer = Pipeline(\n",
    "        [(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        [\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"unknown\")),\n",
    "            (\n",
    "                \"encoder\",\n",
    "                OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Определяем препроцессор. Важно убедиться, что списки features корректны\n",
    "    # и столбцы существуют в X_train\n",
    "    numeric_ft = [f for f in numeric_features if f in X_train.columns]\n",
    "    categorical_ft = [f for f in categorical_features if f in X_train.columns]\n",
    "    other_ft = [\n",
    "        f for f in X_train.columns if f not in numeric_ft and f not in categorical_ft\n",
    "    ]  # Столбцы, которые не в списках\n",
    "\n",
    "    print(\n",
    "        f\"Числовые признаки для пайплайна ({len(numeric_ft)}): {numeric_ft}\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\n",
    "        f\"Категориальные признаки для пайплайна ({len(categorical_ft)}): {categorical_ft}\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    if other_ft:\n",
    "        print(\n",
    "            f\"Признаки не в списках numeric/categorical ({len(other_ft)}). Они будут удалены ColumnTransformer'ом.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"numeric\", numeric_transformer, numeric_ft),\n",
    "            (\"categorical\", categorical_transformer, categorical_ft),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )  # 'drop' означает, что столбцы не из списков features будут удалены\n",
    "\n",
    "    # Инициализация LGBM\n",
    "    lgbm_params = {\n",
    "        \"random_state\": 42,\n",
    "        \"n_estimators\": 150,  # лучший параметр после серии тестов\n",
    "        \"learning_rate\": 0.2,  # лучший параметр после серии тестов\n",
    "        \"max_depth\": 9,  # лучший параметр после серии тестов\n",
    "        \"num_leaves\": 31,  # лучший параметр после серии тестов\n",
    "        \"subsample\": 0.8,  # лучший параметр после серии тестов\n",
    "        \"colsample_bytree\": 0.8,  # лучший параметр после серии тестов\n",
    "        \"reg_alpha\": 0.1,  # лучший параметр после серии тестов\n",
    "        \"reg_lambda\": 0.1,  # лучший параметр после серии тестов\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1,\n",
    "        \"boosting_type\": \"gbdt\",  # Добавим явно, если не указан в lgbm_params\n",
    "    }\n",
    "\n",
    "    # Определяем objective и metric\n",
    "    n_classes = len(y_train.unique())\n",
    "    if n_classes > 2:\n",
    "        lgbm_params[\"objective\"] = \"multiclass\"\n",
    "        lgbm_params[\"metric\"] = \"multi_logloss\"\n",
    "        lgbm_params[\"num_class\"] = (\n",
    "            n_classes  # Указываем количество классов для multiclass\n",
    "        )\n",
    "        print(\n",
    "            f\"Задача: Многоклассовая классификация ({n_classes} классов).\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "    elif n_classes == 2:\n",
    "        lgbm_params[\"objective\"] = \"binary\"\n",
    "        lgbm_params[\"metric\"] = \"auc\"\n",
    "        print(\"Задача: Бинарная классификация.\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\n",
    "            f\"Ошибка: Недостаточно классов ({n_classes}) для обучения классификатора.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    # Дла отладки. Включаем балансировку весов классов для несбалансированных данных, если не указано иное\n",
    "    # Восстановим здесь, т.к. ROC-AUC на несбалансированных данных без этого может быть обманчив\n",
    "    # lgbm_params['class_weight'] = 'balanced' # Добавим, если нужно балансировать\n",
    "\n",
    "    lgbm = LGBMClassifier(**lgbm_params)\n",
    "\n",
    "    # Создаем финальный пайплайн\n",
    "    model_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", lgbm)])\n",
    "\n",
    "    # Определяем индексы категориальных признаков ПОСЛЕ препроцессинга для LGBM\n",
    "    # LGBM может использовать информацию о категориальных признаках напрямую\n",
    "    # Однако, OrdinalEncoder уже преобразует их в числа.\n",
    "\n",
    "    print(\"Обучение модели...\", file=sys.stderr)\n",
    "    try:\n",
    "        model_pipeline.fit(X_train, y_train)\n",
    "        print(\"Обучение завершено.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при обучении модели: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "    # Оценка качества\n",
    "    print(\"Оценка модели...\", file=sys.stderr)\n",
    "    try:\n",
    "        if n_classes > 2:\n",
    "            # Для многоклассовой AUC predict_proba необходим\n",
    "            y_train_pred_proba = model_pipeline.predict_proba(X_train)\n",
    "            y_test_pred_proba = model_pipeline.predict_proba(X_test)\n",
    "            train_score = roc_auc_score(y_train, y_train_pred_proba, multi_class=\"ovr\")\n",
    "            test_score = roc_auc_score(y_test, y_test_pred_proba, multi_class=\"ovr\")\n",
    "        else:  # n_classes == 2\n",
    "            # Для бинарной AUC predict_proba[:, 1] необходим\n",
    "            y_train_pred_proba = model_pipeline.predict_proba(X_train)[:, 1]\n",
    "            y_test_pred_proba = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "            train_score = roc_auc_score(y_train, y_train_pred_proba)\n",
    "            test_score = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "        print(\n",
    "            f\"Train ROC-AUC: {train_score:.4f}, Test ROC-AUC: {test_score:.4f}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        print(f\"Обучено на {n_classes} классах.\", file=sys.stderr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при оценке модели: {e}\", file=sys.stderr)\n",
    "        # Попробуем хотя бы accuracy, если AUC не получается\n",
    "        try:\n",
    "            train_acc = model_pipeline.score(X_train, y_train)\n",
    "            test_acc = model_pipeline.score(X_test, y_test)\n",
    "            print(\n",
    "                f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "        except Exception as e_acc:\n",
    "            print(f\"Не удалось рассчитать даже Accuracy: {e_acc}\", file=sys.stderr)\n",
    "\n",
    "    # Сохранение модели в файл\n",
    "    print(f\"Сохранение модели в файл: {output_file}...\", file=sys.stderr)\n",
    "    try:\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pickle.dump(model_pipeline, f)\n",
    "        print(f\"Модель успешно сохранена.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Ошибка при сохранении модели в файл {output_file}: {e}\", file=sys.stderr\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"--------------------------------------------------------------\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\"Обучение LGBM модели завершено.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"==============================================================================\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3e2f40a-5899-46d9-9e17-2c71c2e8bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение признаков для модели\n",
    "\n",
    "numeric_features = [\n",
    "    \"visit_number\",\n",
    "    \"day_of_week_num\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"utm_source_encoded\",\n",
    "    \"utm_campaign_encoded\",\n",
    "    \"first_hit_number\",\n",
    "    \"last_hit_number\",\n",
    "    \"total_hits\",\n",
    "    \"total_time\",\n",
    "    \"main_referer\",  # Это hit_referer_encoded_<lambda> переименованный в aggregate_session_data\n",
    "    \"main_label\",  # Это event_label_encoded_<lambda> переименованный в aggregate_session_data\n",
    "    \"geo_country\",  # Это бинарный 0/1 признак\n",
    "]\n",
    "\n",
    "\n",
    "categorical_features = [\n",
    "    \"utm_medium\",  # Остается после drop в Блоке 2, используется для fillna и в фичах\n",
    "    \"device_category\",  # Остается после drop в Блоке 2\n",
    "    \"device_brand\",  # Остается после drop в Блоке 2\n",
    "    \"geo_city_grouped\",  # Создано в Блоке 1, остается\n",
    "    \"entry_page\",  # Создано в aggregate_session_data (ранее hit_page_path)\n",
    "    \"main_category_grouped\",  # Создано в Блоке 2, используется в aggregate_session_data, остается\n",
    "    # 'main_action_grouped' # Это целевая переменная\n",
    "]\n",
    "\n",
    "# Убедимся, что целевая переменная не попала в признаки\n",
    "if \"main_action_grouped\" in numeric_features:\n",
    "    numeric_features.remove(\"main_action_grouped\")\n",
    "if \"main_action_grouped\" in categorical_features:\n",
    "    categorical_features.remove(\"main_action_grouped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a056e89-e593-4b05-9f85-53509c35a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Запуск пайплайна обработки и обучения модели.\n",
      "==============================================================================\n",
      "\n",
      "--- Этап 1: Загрузка данных ---\n",
      "Файл ga_hits.pkl успешно загружен.\n",
      "\n",
      "Файл ga_sessions.pkl успешно загружен.\n",
      "Начало обработки 'not set' и 'none'\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Обработка 'not set' и 'none' завершена.\n",
      "-------------------------------------------------------------\n",
      "Начало обработки 'not set' и 'none'\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Обработка 'not set' и 'none' завершена.\n",
      "-------------------------------------------------------------\n",
      "\n",
      "--- Этап 2: Предобработка данных сессий ---\n",
      "Создание признаков даты из visit_date...\n",
      "Признаки даты созданы.\n",
      "Заполнение пропусков в ga_sessions_filled...\n",
      "Начало оптимизированного заполнения пропусков по группам и модой (in-place).\n",
      "--------------------------------------------------------------\n",
      "Заполнение пропусков в 'utm_source' по группам ('utm_medium', 'device_category').\n",
      "Пропусков в 'utm_source' после заполнения по группе: 97\n",
      "Заполнение пропусков в 'utm_campaign' по группам ('utm_medium', 'device_os').\n",
      "Пропусков в 'utm_campaign' после заполнения по группе: 138992\n",
      "Заполнение оставшихся пропусков по более широким группам ['utm_medium', 'device_category', 'device_os'] (для столбцов с пропусками).\n",
      "Финальное заполнение любых оставшихся пропусков общей модой по столбцам.\n",
      "\n",
      "Оптимизированное заполнение пропусков завершено (in-place).\n",
      "-------------------------------------------------------------\n",
      "Пропусков в ga_sessions_filled после fill_missing_by_groups: 0\n",
      "Кодирование utm_source...\n",
      "Кодирование utm_campaign...\n",
      "Кодирование device_model...\n",
      "Преобразование geo_country...\n",
      "Группировка geo_city...\n",
      "Создан 'geo_city_grouped'. Города < 18600 объединены.\n",
      "\n",
      "--- Этап 3: Предобработка и агрегация данных хитов ---\n",
      "Заполнение пропусков в ga_hits_filled...\n",
      "Начало оптимизированного заполнения пропусков в hit_referer и event_label (in-place).\n",
      "--------------------------------------------------------------\n",
      "Заполнение пропусков в 'hit_referer' первым значением в группе по 'hit_page_path'.\n",
      "Пропусков в 'hit_referer' после заполнения по группе: 1600354\n",
      "Заполнение пропусков в 'event_label' первым значением в группе по 'event_action'.\n",
      "Пропусков в 'event_label' после заполнения по группе: 28941\n",
      "Финальное заполнение оставшихся пропусков в 'hit_referer' значением 'direct'.\n",
      "Пропусков в 'hit_referer' после финального заполнения: 0\n",
      "Финальное заполнение оставшихся пропусков в 'event_label' значением 'none'.\n",
      "Пропусков в 'event_label' после финального заполнения: 0\n",
      "\n",
      "Оптимизированное заполнение пропусков в hit_referer и event_label завершено (in-place).\n",
      "-------------------------------------------------------------\n",
      "Пропусков в ga_hits_filled после fill_hits_and_events: 24886813\n",
      "Создание бинарного признака hit_time_2...\n",
      "'hit_time_2' создан.\n",
      "Кодирование hit_referer...\n",
      "Группировка event_category...\n",
      "Создан 'event_category_grouped'. Категории < 157265 объединены.\n",
      "Группировка event_action...\n",
      "Создан 'event_action_grouped'. Действия < 157265 объединены.\n",
      "Кодирование event_label...\n",
      "Удаление столбцов из ga_hits_filled: ['event_value', 'hit_date', 'hit_type', 'hit_time', 'hit_referer', 'event_label']\n",
      "Удаление столбцов из ga_sessions_filled: ['visit_date', 'visit_time', 'utm_source', 'utm_campaign', 'utm_adcontent', 'utm_keyword', 'device_screen_resolution', 'device_os', 'geo_city', 'year']\n",
      "Начало агрегации данных хитов по сессиям.\n",
      "--------------------------------------------------------------\n",
      "Агрегация выполнена.\n",
      "Переименование и финальная обработка столбцов завершена.\n",
      "-------------------------------------------------------------\n",
      "Нет столбцов для удаления из ga_hits_aggregated по списку.\n",
      "\n",
      "--- Этап 4: Объединение датафреймов ---\n",
      "Начало объединения данных.\n",
      "--------------------------------------------------------------\n",
      "Оптимизация типов данных перед объединением...\n",
      "  Оптимизация типов данных...\n",
      "  Память после оптимизации: 314.66 MB (было 976.21 MB)\n",
      "  Оптимизация типов данных...\n",
      "  Память после оптимизации: 412.02 MB (было 653.60 MB)\n",
      "Объединение данных по session_id...\n",
      "Объединение завершено.\n",
      "Оптимизация объединенных данных...\n",
      "  Оптимизация типов данных...\n",
      "  Память после оптимизации: 597.93 MB (было 640.50 MB)\n",
      "Сохранение данных в merged_data.pkl...\n",
      "Файл сохранен. Размер: 354.71 MB\n",
      "Объединение и сохранение завершены.\n",
      "-------------------------------------------------------------\n",
      "Удаление столбцов из объединенного датасета: ['device_model', 'device_browser', 'device_model_encoded']\n",
      "Сохранение финального датасета (перед dropna) в merged_data_2.pkl...\n",
      "Финальный датасет сохранен в merged_data_2.pkl.\n",
      "Debug: Проверка наличия целевого столбца 'main_action_grouped' в data.columns перед dropna...\n",
      "Debug: 'main_action_grouped' in data.columns (before dropna): True\n",
      "Debug: data['main_action_grouped'].isnull().sum() (before dropna): 127776\n",
      "Debug: data['main_action_grouped'].value_counts().head() (before dropna):\n",
      "main_action_grouped\n",
      "sub_landing            714427\n",
      "view_card              376372\n",
      "view_new_card          181149\n",
      "OTHER                  169769\n",
      "sub_view_cars_click     78057\n",
      "Name: count, dtype: int64\n",
      "Удалено 127776 строк с NaN в 'first_hit_number'.\n",
      "Debug: Проверка наличия целевого столбца 'main_action_grouped' в data.columns после dropna...\n",
      "Debug: 'main_action_grouped' in data.columns (after dropna): True\n",
      "Debug: data['main_action_grouped'].isnull().sum() (after dropna): 0\n",
      "Debug: data['main_action_grouped'].value_counts().head() (after dropna):\n",
      "main_action_grouped\n",
      "sub_landing            714427\n",
      "view_card              376372\n",
      "view_new_card          181149\n",
      "OTHER                  169769\n",
      "sub_view_cars_click     78057\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Этап 5: Обучение модели ---\n",
      "\n",
      "==============================================================================\n",
      "Начало обучения LGBM модели.\n",
      "--------------------------------------------------------------\n",
      "Debug: Проверка наличия целевого столбца 'main_action_grouped' в data.columns в начале train_and_save_lgbm_model...\n",
      "Debug: 'main_action_grouped' in data.columns: True\n",
      "Редкие классы не найдены (все классы >= min_samples).\n",
      "Обучение будет проходить на 1732266 образцах после фильтрации.\n",
      "Данные разделены на train (1385812) и test (346454).\n",
      "Числовые признаки для пайплайна (13): ['visit_number', 'day_of_week_num', 'month', 'day', 'utm_source_encoded', 'utm_campaign_encoded', 'first_hit_number', 'last_hit_number', 'total_hits', 'total_time', 'main_referer', 'main_label', 'geo_country']\n",
      "Категориальные признаки для пайплайна (6): ['utm_medium', 'device_category', 'device_brand', 'geo_city_grouped', 'entry_page', 'main_category_grouped']\n",
      "Признаки не в списках numeric/categorical (1). Они будут удалены ColumnTransformer'ом.\n",
      "Задача: Многоклассовая классификация (18 классов).\n",
      "Обучение модели...\n",
      "Обучение завершено.\n",
      "Оценка модели...\n",
      "Ошибка при оценке модели: name 'roc_auc_score' is not defined\n",
      "Train Accuracy: 0.8075, Test Accuracy: 0.8028\n",
      "Сохранение модели в файл: lgbm_model.pkl...\n",
      "Модель успешно сохранена.\n",
      "--------------------------------------------------------------\n",
      "Обучение LGBM модели завершено.\n",
      "==============================================================================\n",
      "\n",
      "==============================================================================\n",
      "Пайплайн обработки и обучения модели завершен.\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Основной исполняемый код (Пайплайн)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Запуск пайплайна обработки и обучения модели.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"==============================================================================\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # Настройка путей к данным\n",
    "    file_path_hits = \"ga_hits.pkl\"\n",
    "    file_path_sessions = \"ga_sessions.pkl\"\n",
    "    output_merged_pkl_1 = \"merged_data.pkl\"\n",
    "    output_merged_pkl_2 = \"merged_data_2.pkl\"  # Файл после финал.удаления столбцов\n",
    "    output_model_pkl = \"lgbm_model.pkl\"\n",
    "    target_column_name = \"main_action_grouped\"  # Целевая переменная\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Этап 1: Загрузка данных\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n--- Этап 1: Загрузка данных ---\", file=sys.stderr)\n",
    "    hits = None\n",
    "    sessions = None\n",
    "\n",
    "    try:\n",
    "        hits = pd.read_pickle(file_path_hits)\n",
    "        print(f\"Файл {file_path_hits} успешно загружен.\", file=sys.stderr)\n",
    "        # print(hits.head()) # EDA/Отладка\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: Файл {file_path_hits} не найден.\", file=sys.stderr)\n",
    "        sys.exit(1)  # Выход с ошибкой, если файл не найден\n",
    "    except Exception as e:\n",
    "        print(f\"Не удалось загрузить {file_path_hits}: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        sessions = pd.read_pickle(file_path_sessions)\n",
    "        print(f\"\\nФайл {file_path_sessions} успешно загружен.\", file=sys.stderr)\n",
    "        # print(sessions.head()) # EDA/Отладка\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Ошибка: Файл {file_path_sessions} не найден.\", file=sys.stderr)\n",
    "        sys.exit(1)  # Выход с ошибкой\n",
    "    except Exception as e:\n",
    "        print(f\"Не удалось загрузить {file_path_sessions}: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Копируем датафреймы для работы\n",
    "    ga_hits = hits.copy()\n",
    "    ga_sessions = sessions.copy()\n",
    "    del hits, sessions  # Освобождаем память от исходных\n",
    "    gc.collect()\n",
    "\n",
    "    # Применяем начальную предобработку 'not set'/'none' (Из Блока 1)\n",
    "    process_notset_none(ga_sessions)\n",
    "    process_notset_none(ga_hits)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Этап 2: Предобработка данных сессий (ga_sessions_filled)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\n--- Этап 2: Предобработка данных сессий ---\", file=sys.stderr)\n",
    "\n",
    "    ga_sessions_filled = ga_sessions.copy()\n",
    "    del ga_sessions\n",
    "    gc.collect()\n",
    "\n",
    "    # visit_date → разобьём на день, месяц и год + выделим день недели (Из Блока 1)\n",
    "    print(\"Создание признаков даты из visit_date...\", file=sys.stderr)\n",
    "    if \"visit_date\" in ga_sessions_filled.columns:\n",
    "        try:\n",
    "            ga_sessions_filled[\"visit_date\"] = pd.to_datetime(\n",
    "                ga_sessions_filled[\"visit_date\"]\n",
    "            )\n",
    "            ga_sessions_filled[\"year\"] = ga_sessions_filled[\n",
    "                \"visit_date\"\n",
    "            ].dt.year.astype(\"int16\")\n",
    "            ga_sessions_filled[\"month\"] = ga_sessions_filled[\n",
    "                \"visit_date\"\n",
    "            ].dt.month.astype(\"int16\")\n",
    "            ga_sessions_filled[\"day\"] = ga_sessions_filled[\"visit_date\"].dt.day.astype(\n",
    "                \"int16\"\n",
    "            )\n",
    "            ga_sessions_filled[\"day_of_week_num\"] = ga_sessions_filled[\n",
    "                \"visit_date\"\n",
    "            ].dt.dayofweek.astype(\"int16\")\n",
    "            print(\"Признаки даты созданы.\", file=sys.stderr)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при создании признаков даты: {e}\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\"'visit_date' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # utm_medium device_category device_os → заполним пропуски\n",
    "    print(\"Заполнение пропусков в ga_sessions_filled...\", file=sys.stderr)\n",
    "    ga_sessions_filled = fill_missing_by_groups(ga_sessions_filled)\n",
    "    print(\n",
    "        f\"Пропусков в ga_sessions_filled после fill_missing_by_groups: {ga_sessions_filled.isna().sum().sum()}\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # utm_source → заменим хэш коды на числа\n",
    "    if \"utm_source\" in ga_sessions_filled.columns:\n",
    "        print(\"Кодирование utm_source...\", file=sys.stderr)\n",
    "        ga_sessions_filled = encode_column_with_nulls(ga_sessions_filled, \"utm_source\")\n",
    "    else:\n",
    "        print(\"'utm_source' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # utm_campaign → заменим хэш коды на числа\n",
    "    if \"utm_campaign\" in ga_sessions_filled.columns:\n",
    "        print(\"Кодирование utm_campaign...\", file=sys.stderr)\n",
    "        ga_sessions_filled = encode_column_with_nulls(\n",
    "            ga_sessions_filled, \"utm_campaign\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"'utm_campaign' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # device_model → заменим хэш коды на числа\n",
    "    if \"device_model\" in ga_sessions_filled.columns:\n",
    "        print(\"Кодирование device_model...\", file=sys.stderr)\n",
    "        ga_sessions_filled = encode_column_with_nulls(\n",
    "            ga_sessions_filled, \"device_model\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"'device_model' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # geo_country → заменим Россию на 1, а остальные значения на 0\n",
    "    if \"geo_country\" in ga_sessions_filled.columns:\n",
    "        print(\"Преобразование geo_country...\", file=sys.stderr)\n",
    "        ga_sessions_filled[\"geo_country\"] = ga_sessions_filled[\"geo_country\"].apply(\n",
    "            lambda x: 1 if x == \"Russia\" else 0\n",
    "        )\n",
    "    else:\n",
    "        print(\"'geo_country' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # geo_city → преобразуем города с малым количеством вхождений в категорию иных\n",
    "    if \"geo_city\" in ga_sessions_filled.columns:\n",
    "        print(\"Группировка geo_city...\", file=sys.stderr)\n",
    "        threshold = len(ga_sessions_filled) * 0.01\n",
    "        value_counts = ga_sessions_filled[\"geo_city\"].value_counts()\n",
    "        replace_dict = {\n",
    "            category: \"OTHER\"\n",
    "            for category in value_counts[value_counts < threshold].index\n",
    "        }\n",
    "        ga_sessions_filled[\"geo_city_grouped\"] = ga_sessions_filled[\"geo_city\"].replace(\n",
    "            replace_dict\n",
    "        )\n",
    "        print(\n",
    "            f\"Создан 'geo_city_grouped'. Города < {threshold:.0f} объединены.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "        # import plotly.express as px # Импорт для EDA\n",
    "        # # EDA/Визуализация - опционально для деплоя\n",
    "        # try:\n",
    "        #      city_counts = ga_sessions_filled['geo_city_grouped'].value_counts().reset_index()\n",
    "        #      city_counts.columns = ['city', 'count']\n",
    "        #      fig = px.bar(city_counts, x='city', y='count', title=f'Распределение городов (города < {threshold:,.0f} событий объединены в OTHER)')\n",
    "        #      # fig.show() # Показать график - требует интерактивной среды\n",
    "        #      print(\"График распределения городов готов (EDA).\", file=sys.stderr)\n",
    "        # except Exception as e:\n",
    "        #      print(f\"Не удалось построить график geo_city: {e}\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\"'geo_city' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # print(ga_sessions_filled.head()) # Отладка\n",
    "    # print(ga_sessions_filled.isna().sum()) # Отладка\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Этап 3: Предобработка и агрегация данных хитов (ga_hits_aggregated)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\n--- Этап 3: Предобработка и агрегация данных хитов ---\", file=sys.stderr)\n",
    "\n",
    "    ga_hits_filled = ga_hits.copy()\n",
    "    del ga_hits\n",
    "    gc.collect()\n",
    "\n",
    "    # Заполнение пропусков на основе группировки в hits (Из Блока 2)\n",
    "    print(\"Заполнение пропусков в ga_hits_filled...\", file=sys.stderr)\n",
    "    ga_hits_filled = fill_hits_and_events(ga_hits_filled)\n",
    "    print(\n",
    "        f\"Пропусков в ga_hits_filled после fill_hits_and_events: {ga_hits_filled.isna().sum().sum()}\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # hit_time → учитывая количество пропусков (перезодов без действий) преобразуем в есть/нет действий\n",
    "    if \"hit_time\" in ga_hits_filled.columns:\n",
    "        print(\"Создание бинарного признака hit_time_2...\", file=sys.stderr)\n",
    "        ga_hits_filled[\"hit_time_2\"] = ga_hits_filled[\"hit_time\"].notna().astype(int)\n",
    "        print(\"'hit_time_2' создан.\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\n",
    "            \"'hit_time' столбец отсутствует, не удалось создать 'hit_time_2'.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # hit_referer → преобразуем хэш коды в цифры\n",
    "    if \"hit_referer\" in ga_hits_filled.columns:\n",
    "        print(\"Кодирование hit_referer...\", file=sys.stderr)\n",
    "        ga_hits_filled = encode_column_with_nulls(ga_hits_filled, \"hit_referer\")\n",
    "    else:\n",
    "        print(\"'hit_referer' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # event_category → заменим малозначимые параметры на иные\n",
    "    if \"event_category\" in ga_hits_filled.columns:\n",
    "        print(\"Группировка event_category...\", file=sys.stderr)\n",
    "        threshold = len(ga_hits_filled) * 0.01\n",
    "        value_counts = ga_hits_filled[\"event_category\"].value_counts()\n",
    "        replace_dict = {\n",
    "            category: \"OTHER\"\n",
    "            for category in value_counts[value_counts < threshold].index\n",
    "        }\n",
    "        ga_hits_filled[\"event_category_grouped\"] = ga_hits_filled[\n",
    "            \"event_category\"\n",
    "        ].replace(replace_dict)\n",
    "        print(\n",
    "            f\"Создан 'event_category_grouped'. Категории < {threshold:.0f} объединены.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "        # import plotly.express as px # Импорт для EDA\n",
    "        # # EDA/Визуализация - опционально для деплоя\n",
    "        # try:\n",
    "        #      event_counts = ga_hits_filled['event_category_grouped'].value_counts().reset_index()\n",
    "        #      event_counts.columns = ['category', 'count']\n",
    "        #      fig = px.bar(event_counts, x='category', y='count', title=f'Распределение категорий событий (категории < {threshold:,.0f} событий объединены в OTHER)')\n",
    "        #      # fig.show() # Показать график\n",
    "        #      print(\"График распределения категорий событий готов (EDA).\", file=sys.stderr)\n",
    "        # except Exception as e:\n",
    "        #      print(f\"Не удалось построить график event_category: {e}\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\"'event_category' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # event_action → заменим малозначимые параметры на иные\n",
    "    if \"event_action\" in ga_hits_filled.columns:\n",
    "        print(\"Группировка event_action...\", file=sys.stderr)\n",
    "        threshold = len(ga_hits_filled) * 0.01\n",
    "        value_counts = ga_hits_filled[\"event_action\"].value_counts()\n",
    "        replace_dict = {\n",
    "            category: \"OTHER\"\n",
    "            for category in value_counts[value_counts < threshold].index\n",
    "        }\n",
    "        ga_hits_filled[\"event_action_grouped\"] = ga_hits_filled[\"event_action\"].replace(\n",
    "            replace_dict\n",
    "        )\n",
    "        print(\n",
    "            f\"Создан 'event_action_grouped'. Действия < {threshold:.0f} объединены.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        # import plotly.express as px # Импорт для EDA\n",
    "        # # EDA/Визуализация - опционально для деплоя\n",
    "        # try:\n",
    "        #      action_counts = ga_hits_filled['event_action_grouped'].value_counts().reset_index()\n",
    "        #      action_counts.columns = ['action', 'count']\n",
    "        #      fig = px.bar(action_counts, x='action', y='count', title=f'Распределение действий (действия < {threshold:,.0f} событий объединены в OTHER)')\n",
    "        #      # fig.show() # Показать график\n",
    "        #      print(\"График распределения действий готов (EDA).\", file=sys.stderr)\n",
    "        # except Exception as e:\n",
    "        #      print(f\"Не удалось построить график event_action: {e}\", file=sys.stderr)\n",
    "    else:\n",
    "        print(\"'event_action' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # event_label → преобразуем хэш коды в цифры\n",
    "    if \"event_label\" in ga_hits_filled.columns:\n",
    "        print(\"Кодирование event_label...\", file=sys.stderr)\n",
    "        ga_hits_filled = encode_column_with_nulls(ga_hits_filled, \"event_label\")\n",
    "    else:\n",
    "        print(\"'event_label' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    # Удаляем неактуальные для анализа столбцы из ga_hits_filled перед агрегацией\n",
    "    # Порядок важен: сначала создаем новые признаки, потом удаляем старые.\n",
    "    # Новые признаки используются в агрегации.\n",
    "    columns_to_drop_from_hits_filled = [\n",
    "        \"event_value\",\n",
    "        \"hit_date\",\n",
    "        \"hit_type\",\n",
    "        \"hit_time\",\n",
    "        \"hit_referer\",\n",
    "        \"event_label\",\n",
    "    ]\n",
    "    existing_cols_to_drop_hits = [\n",
    "        col for col in columns_to_drop_from_hits_filled if col in ga_hits_filled.columns\n",
    "    ]\n",
    "    if existing_cols_to_drop_hits:\n",
    "        print(\n",
    "            f\"Удаление столбцов из ga_hits_filled: {existing_cols_to_drop_hits}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        ga_hits_filled = ga_hits_filled.drop(columns=existing_cols_to_drop_hits, axis=1)\n",
    "    else:\n",
    "        print(\"Нет столбцов для удаления из ga_hits_filled по списку.\", file=sys.stderr)\n",
    "\n",
    "    # Удаляем неактуальные для анализа столбцы из ga_sessions_filled (Из Блока 2)\n",
    "    columns_to_drop_from_sessions_filled = [\n",
    "        \"visit_date\",\n",
    "        \"visit_time\",\n",
    "        \"utm_source\",\n",
    "        \"utm_campaign\",\n",
    "        \"utm_adcontent\",\n",
    "        \"utm_keyword\",\n",
    "        \"device_screen_resolution\",\n",
    "        \"device_os\",\n",
    "        \"geo_city\",\n",
    "        \"year\",\n",
    "    ]\n",
    "    existing_cols_to_drop_sessions = [\n",
    "        col\n",
    "        for col in columns_to_drop_from_sessions_filled\n",
    "        if col in ga_sessions_filled.columns\n",
    "    ]\n",
    "    if existing_cols_to_drop_sessions:\n",
    "        print(\n",
    "            f\"Удаление столбцов из ga_sessions_filled: {existing_cols_to_drop_sessions}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        ga_sessions_filled = ga_sessions_filled.drop(\n",
    "            columns=existing_cols_to_drop_sessions, axis=1\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Нет столбцов для удаления из ga_sessions_filled по списку.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # Агрегация ga_hits_filled до уровня сессий\n",
    "    ga_hits_aggregated = aggregate_session_data(ga_hits_filled)\n",
    "    del ga_hits_filled\n",
    "    gc.collect()\n",
    "    # print(ga_hits_aggregated.head()) # Отладка\n",
    "\n",
    "    # Удаляем неактуальные после агрегации столбцы из ga_hits_aggregated\n",
    "    columns_to_drop_from_hits_aggregated = [\n",
    "        \"main_category\",\n",
    "        \"main_action\",\n",
    "    ]  # Это агрегированные версии ОРИГИНАЛОВ, которые были удалены ранее.\n",
    "    # В aggregate_session_data агрегация для них используется,\n",
    "    # но затем они удаляются. Оставим как есть по коду.\n",
    "    existing_cols_to_drop_hits_agg = [\n",
    "        col\n",
    "        for col in columns_to_drop_from_hits_aggregated\n",
    "        if col in ga_hits_aggregated.columns\n",
    "    ]\n",
    "    if existing_cols_to_drop_hits_agg:\n",
    "        print(\n",
    "            f\"Удаление столбцов из ga_hits_aggregated: {existing_cols_to_drop_hits_agg}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        ga_hits_aggregated = ga_hits_aggregated.drop(\n",
    "            columns=existing_cols_to_drop_hits_agg, axis=1\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Нет столбцов для удаления из ga_hits_aggregated по списку.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # EDA/Анализ агрегированных данных - опционально для деплоя\n",
    "    # analyze_dataframe_columns(ga_hits_aggregated)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Этап 4: Объединение датафреймов (Из Блока 3)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\n--- Этап 4: Объединение датафреймов ---\", file=sys.stderr)\n",
    "\n",
    "    # Объединим оба датафрейма в единый датасет\n",
    "    data = merge_and_save(\n",
    "        ga_sessions_filled, ga_hits_aggregated, output_pkl=output_merged_pkl_1\n",
    "    )\n",
    "    del ga_sessions_filled, ga_hits_aggregated\n",
    "    gc.collect()\n",
    "\n",
    "    if data.empty:\n",
    "        print(\n",
    "            \"Ошибка: Объединенный датафрейм пуст. Дальнейшая обработка невозможна.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        sys.exit(1)\n",
    "\n",
    "    # EDA/Анализ объединенного датасета - опционально для деплоя\n",
    "    # print(\"\\nОбъединенные данные (до финального дропа):\", file=sys.stderr)\n",
    "    # print(data.info(memory_usage='deep'), file=sys.stderr)\n",
    "    # print(data.head(), file=sys.stderr)\n",
    "\n",
    "    # Удаляем неактуальные столбцы из объединенного датасета\n",
    "    columns_to_drop_from_data = [\n",
    "        \"device_model\",\n",
    "        \"device_browser\",\n",
    "        \"device_model_encoded\",\n",
    "    ]\n",
    "    existing_cols_to_drop_data = [\n",
    "        col for col in columns_to_drop_from_data if col in data.columns\n",
    "    ]\n",
    "    if existing_cols_to_drop_data:\n",
    "        print(\n",
    "            f\"Удаление столбцов из объединенного датасета: {existing_cols_to_drop_data}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        data = data.drop(columns=existing_cols_to_drop_data, axis=1)\n",
    "    else:\n",
    "        print(\n",
    "            \"Нет столбцов для удаления из объединенного датасета по списку.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    # Сохраняем итоговый датасет в pkl файл\n",
    "    # Этот файл уже содержит финальный набор признаков после частичного дропа\n",
    "    # Перед финальным удалением строк.\n",
    "    print(\n",
    "        f\"Сохранение финального датасета (перед dropna) в {output_merged_pkl_2}...\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    try:\n",
    "        with open(output_merged_pkl_2, \"wb\") as f:\n",
    "            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Финальный датасет сохранен в {output_merged_pkl_2}.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Ошибка при сохранении файла {output_merged_pkl_2}: {e}\", file=sys.stderr\n",
    "        )\n",
    "\n",
    "    # EDA/Матрица корреляции - опционально для деплоя\n",
    "    # try:\n",
    "    #      print(\"Построение матрицы корреляции (EDA)...\", file=sys.stderr)\n",
    "    #      # Выбираем только числовые столбцы, которые точно есть в data\n",
    "    #      # Используем список numeric_features как ориентир, но проверяем по data.columns\n",
    "    #      # Добавим session_id, т.к. он числовой, но обычно не участвует в корреляции признаков\n",
    "    #      cols_for_corr = [col for col in data.select_dtypes(include=np.number).columns if col != 'session_id']\n",
    "    #      # Убедимся, что целевая переменная тоже не в корреляции признаков, если она числовая\n",
    "    #      if target_column_name in cols_for_corr:\n",
    "    #          cols_for_corr.remove(target_column_name)\n",
    "\n",
    "    #      if len(cols_for_corr) > 1:\n",
    "    #           correlation_matrix = data[cols_for_corr].corr(method='pearson')\n",
    "    #           mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    #           f, ax = plt.subplots(figsize=(11, 9))\n",
    "    #           cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    #           sns.heatmap(correlation_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "    #                        square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    #           # plt.show() # Показать график\n",
    "    #           print(\"Матрица корреляции построена (EDA).\", file=sys.stderr)\n",
    "    #      else:\n",
    "    #          print(\"Недостаточно числовых столбцов для построения матрицы корреляции.\", file=sys.stderr)\n",
    "\n",
    "    # except Exception as e:\n",
    "    #      print(f\"Не удалось построить матрицу корреляции: {e}\", file=sys.stderr)\n",
    "\n",
    "    # === ОТЛАДОЧНЫЙ ПРИНТ: Проверка наличия целевого столбца перед dropna ===\n",
    "    print(\n",
    "        f\"Debug: Проверка наличия целевого столбца '{target_column_name}' в data.columns перед dropna...\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\n",
    "        f\"Debug: '{target_column_name}' in data.columns (before dropna): {target_column_name in data.columns}\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    if target_column_name in data.columns:\n",
    "        print(\n",
    "            f\"Debug: data['{target_column_name}'].isnull().sum() (before dropna): {data[target_column_name].isnull().sum()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        print(\n",
    "            f\"Debug: data['{target_column_name}'].value_counts().head() (before dropna):\\n{data[target_column_name].value_counts().head()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "    # =======================================================================\n",
    "\n",
    "    # Удаляем действия без пользователей (строки с NaN в 'first_hit_number')\n",
    "    # Это финальный шаг очистки данных перед обучением\n",
    "    if \"first_hit_number\" in data.columns:\n",
    "        initial_rows = len(data)\n",
    "        data = data.dropna(subset=[\"first_hit_number\"])\n",
    "        if len(data) < initial_rows:\n",
    "            print(\n",
    "                f\"Удалено {initial_rows - len(data)} строк с NaN в 'first_hit_number'.\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"'first_hit_number' не содержит NaN или столбец отсутствует (строки не удалены).\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "    else:\n",
    "        print(\n",
    "            \"'first_hit_number' столбец отсутствует, строки не удалены по этому критерию.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "\n",
    "    if data.empty:\n",
    "        print(\"Ошибка: Датафрейм пуст после финальной очистки.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # === ОТЛАДОЧНЫЙ ПРИНТ: Проверка наличия целевого столбца после dropna ===\n",
    "    print(\n",
    "        f\"Debug: Проверка наличия целевого столбца '{target_column_name}' в data.columns после dropna...\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\n",
    "        f\"Debug: '{target_column_name}' in data.columns (after dropna): {target_column_name in data.columns}\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    if target_column_name in data.columns:\n",
    "        print(\n",
    "            f\"Debug: data['{target_column_name}'].isnull().sum() (after dropna): {data[target_column_name].isnull().sum()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        print(\n",
    "            f\"Debug: data['{target_column_name}'].value_counts().head() (after dropna):\\n{data[target_column_name].value_counts().head()}\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "    # ======================================================================\n",
    "\n",
    "    # print(\"\\nФинальные данные перед обучением:\", file=sys.stderr)\n",
    "    # print(data.info(memory_usage='deep'), file=sys.stderr)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Этап 5: Обучение модели LGBM (Из Блока 5)\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\n--- Этап 5: Обучение модели ---\", file=sys.stderr)\n",
    "\n",
    "    # Обучение модели LGBM и сохранение\n",
    "    # Используем функцию train_and_save_lgbm_model из Блока 5\n",
    "    trained_pipeline = train_and_save_lgbm_model(\n",
    "        data,\n",
    "        target_column=target_column_name,\n",
    "        output_file=output_model_pkl,\n",
    "        min_class_samples=5,  # Параметр фильтрации редких классов\n",
    "    )\n",
    "\n",
    "    if trained_pipeline is None:\n",
    "        print(\"Обучение модели не удалось.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # EDA/Пример предсказаний после обучения - опционально для деплоя\n",
    "    # print(\"\\nПример предсказаний на всем датасете:\", file=sys.sys.stderr)\n",
    "    # try:\n",
    "    #      # Предсказания (только для известных классов, т.к. редкие были отфильтрованы при обучении)\n",
    "    #      # predict возвращает предсказанный класс\n",
    "    #      predictions = trained_pipeline.predict(data)\n",
    "    #      print(\"Первые 10 предсказанных классов:\", predictions[:10], file=sys.stderr)\n",
    "\n",
    "    #      # predict_proba возвращает вероятности для каждого класса\n",
    "    #      # probabilities = trained_pipeline.predict_proba(data)\n",
    "    #      # print(\"Первые 10 предсказанных вероятностей:\", probabilities[:10], file=sys.stderr)\n",
    "\n",
    "    # except Exception as e:\n",
    "    #      print(f\"Ошибка при выполнении примера предсказаний: {e}\", file=sys.stderr)\n",
    "\n",
    "    # Пример загрузки модели из файла (опционально для деплоя/инференса)\n",
    "    # print(f\"\\nПример загрузки модели из {output_model_pkl}...\", file=sys.stderr)\n",
    "    # loaded_model = None\n",
    "    # try:\n",
    "    #     with open(output_model_pkl, 'rb') as f:\n",
    "    #          loaded_model = pickle.load(f)\n",
    "    #     print(\"Модель успешно загружена.\", file=sys.stderr)\n",
    "\n",
    "    #     # Пример использования загруженной модели для предсказания на новых данных\n",
    "    #     # (здесь просто используем те же данные 'data' для демонстрации)\n",
    "    #     # if loaded_model is not None:\n",
    "    #     #      print(\"\\nПредсказания с загруженной моделью (первые 5):\", file=sys.stderr)\n",
    "    #     #      loaded_predictions = loaded_model.predict(data.head())\n",
    "    #     #      print(loaded_predictions, file=sys.stderr)\n",
    "\n",
    "    # except FileNotFoundError:\n",
    "    #     print(f\"Ошибка: Файл модели {output_model_pkl} не найден для загрузки.\", file=sys.stderr)\n",
    "    # except Exception as e:\n",
    "    #      print(f\"Ошибка при загрузке модели из {output_model_pkl}: {e}\", file=sys.stderr)\n",
    "\n",
    "    print(\n",
    "        \"\\n==============================================================================\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\"Пайплайн обработки и обучения модели завершен.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"==============================================================================\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "\n",
    "# Примечание по деплою:\n",
    "# Для развертывания (деплоя) этого скрипта вам потребуется:\n",
    "# 1. Установить все необходимые библиотеки (pandas, numpy, sklearn, lightgbm).\n",
    "# 2. Убедиться, что файлы данных ('ga_hits.pkl', 'ga_sessions.pkl') доступны по указанным путям.\n",
    "# 3. Убедиться, что скрипт имеет права на запись файлов ('merged_data.pkl', 'merged_data_2.pkl', 'lgbm_model.pkl').\n",
    "# 4. Для инференса на новых данных, вам потребуется загрузить сохраненную модель ('lgbm_model.pkl')\n",
    "#    и применить к новым данным ту же последовательность предобработки, что была использована перед обучением.\n",
    "#    Часть кода для загрузки модели и примера предсказаний закомментирована в конце скрипта.\n",
    "# 5. Возможно, потребуется параметризовать входные/выходные пути и параметры модели\n",
    "#    (например, через аргументы командной строки или файл конфигурации) для удобства развертывания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2b4e7-391d-4487-8ebc-b0547bc289c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf43022-4ed4-49bf-a46f-7e77b0197edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e456a3dc-4121-441d-bba5-e20d9730d700",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (170577031.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Здесь пауза. строка в формате кода, так что при автоматическом воспроизведении выдаст ошибку\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Здесь пауза. строка в формате кода, так что при автоматическом воспроизведении выдаст ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9ce82-9290-450b-bc9e-5379acccd995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39af39-79f8-47a3-b37f-8f3881c1bc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f1b10-feb9-477e-ac07-cee1e79023a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скрипт для выполнения предсказания на валидационных данных\n",
    "# с замером времени.\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. Импорт необходимых библиотек\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time  # Для замера времени\n",
    "import sys\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# Импорты sklearn (необходимы для функций предобработки и загрузки модели)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    ")  # Хотя train_test_split не используется в предсказании, он часть preprocess_data\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    ")  # Не используется для предсказания, но может быть в загруженной модели\n",
    "\n",
    "# Импорт LGBMClassifier (необходим для загрузки модели)\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Определение вспомогательных функций (Скопированы из полного пайплайна)\n",
    "#    Примечание: Эти функции нужны для работы preprocess_data.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def process_notset_none(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Обнаруживает строки 'not set' и 'none' и заменяет на np.nan. In-place.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        return  # Добавлена проверка\n",
    "    strings_to_find_and_replace = [\"not set\", \"none\"]\n",
    "    # print(\"Начало обработки 'not set' и 'none'\", file=sys.stderr)\n",
    "    for col in df.columns:\n",
    "        col_series_str = df[col].astype(str)\n",
    "        combined_mask = col_series_str.str.contains(\n",
    "            strings_to_find_and_replace[0], case=False, regex=False\n",
    "        ) | col_series_str.str.contains(\n",
    "            strings_to_find_and_replace[1], case=False, regex=False\n",
    "        )\n",
    "        if combined_mask.sum() > 0:\n",
    "            df.loc[combined_mask, col] = np.nan\n",
    "    # print(\"Обработка 'not set' и 'none' завершена.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def encode_column_with_nulls(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Кодирует столбец числовыми метками (NaN->0, остальное 1+). Возвращает новый DF.\"\"\"\n",
    "    df = df.copy()\n",
    "    if column_name not in df.columns:\n",
    "        return df\n",
    "    encoded, uniques = pd.factorize(df[column_name].astype(str))\n",
    "    df[f\"{column_name}_encoded\"] = np.where(encoded == -1, 0, encoded + 1)\n",
    "    df[f\"{column_name}_encoded\"] = df[f\"{column_name}_encoded\"].astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_mode(x):\n",
    "    \"\"\"Вспомогательная функция для моды.\"\"\"\n",
    "    mode_val = x.mode()\n",
    "    if not mode_val.empty:\n",
    "        return mode_val[0]\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def fill_missing_by_groups(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Заполняет пропуски по группам и модой. In-place.\"\"\"\n",
    "    # print(\"Начало заполнения пропусков по группам...\", file=sys.stderr)\n",
    "    if (\n",
    "        \"utm_source\" in df.columns\n",
    "        and \"utm_medium\" in df.columns\n",
    "        and \"device_category\" in df.columns\n",
    "        and df[\"utm_source\"].isna().any()\n",
    "    ):\n",
    "        group_modes_source = df.groupby([\"utm_medium\", \"device_category\"])[\n",
    "            \"utm_source\"\n",
    "        ].transform(get_mode)\n",
    "        df[\"utm_source\"].fillna(group_modes_source, inplace=True)\n",
    "    if (\n",
    "        \"utm_campaign\" in df.columns\n",
    "        and \"utm_medium\" in df.columns\n",
    "        and \"device_os\" in df.columns\n",
    "        and df[\"utm_campaign\"].isna().any()\n",
    "    ):\n",
    "        group_modes_campaign = df.groupby([\"utm_medium\", \"device_os\"])[\n",
    "            \"utm_campaign\"\n",
    "        ].transform(get_mode)\n",
    "        df[\"utm_campaign\"].fillna(group_modes_campaign, inplace=True)\n",
    "\n",
    "    group_columns_broad = [\"utm_medium\", \"device_category\", \"device_os\"]\n",
    "    if all(col in df.columns for col in group_columns_broad):\n",
    "        cols_with_nan_after_specific = df.columns[df.isna().any()].tolist()\n",
    "        for col in cols_with_nan_after_specific:\n",
    "            if col in [\"utm_source\", \"utm_campaign\"] and not df[col].isna().any():\n",
    "                continue\n",
    "            group_modes_broad_col = df.groupby(group_columns_broad)[col].transform(\n",
    "                get_mode\n",
    "            )\n",
    "            df[col].fillna(group_modes_broad_col, inplace=True)\n",
    "\n",
    "    cols_with_nan_final = df.columns[df.isna().any()].tolist()\n",
    "    for col in cols_with_nan_final:\n",
    "        mode_val = df[col].mode()\n",
    "        if not mode_val.empty:\n",
    "            df[col].fillna(mode_val[0], inplace=True)\n",
    "    # print(\"Заполнение пропусков завершено.\", file=sys.stderr)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_hits_and_events(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Заполняет пропуски в hit_referer и event_label. In-place.\"\"\"\n",
    "    # print(\"Начало заполнения пропусков в hits...\", file=sys.stderr)\n",
    "    if (\n",
    "        \"hit_referer\" in df.columns\n",
    "        and \"hit_page_path\" in df.columns\n",
    "        and df[\"hit_referer\"].isna().any()\n",
    "    ):\n",
    "        referer_by_page = df.groupby(\"hit_page_path\")[\"hit_referer\"].transform(\"first\")\n",
    "        df[\"hit_referer\"].fillna(referer_by_page, inplace=True)\n",
    "    if (\n",
    "        \"event_label\" in df.columns\n",
    "        and \"event_action\" in df.columns\n",
    "        and df[\"event_label\"].isna().any()\n",
    "    ):\n",
    "        label_by_action = df.groupby(\"event_action\")[\"event_label\"].transform(\"first\")\n",
    "        df[\"event_label\"].fillna(label_by_action, inplace=True)\n",
    "    if \"hit_referer\" in df.columns and df[\"hit_referer\"].isna().any():\n",
    "        df[\"hit_referer\"].fillna(\"direct\", inplace=True)\n",
    "    if \"event_label\" in df.columns and df[\"event_label\"].isna().any():\n",
    "        df[\"event_label\"].fillna(\"none\", inplace=True)\n",
    "    # print(\"Заполнение пропусков в hits завершено.\", file=sys.stderr)\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_session_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Агрегирует данные хитов до уровня сессий.\"\"\"\n",
    "    # print(\"Начало агрегации данных хитов...\", file=sys.stderr)\n",
    "    if \"session_id\" not in df.columns:\n",
    "        print(\"Ошибка: 'session_id' отсутствует для агрегации.\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    agg_rules = {\n",
    "        \"hit_number\": [\"min\", \"max\", \"count\"],\n",
    "        **(\n",
    "            {\"hit_page_path\": lambda x: x.iloc[0] if not x.empty else np.nan}\n",
    "            if \"hit_page_path\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "        **(\n",
    "            {\n",
    "                \"event_category_grouped\": lambda x: (\n",
    "                    x.value_counts().index[0] if not x.empty else np.nan\n",
    "                )\n",
    "            }\n",
    "            if \"event_category_grouped\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "        **(\n",
    "            {\n",
    "                \"event_action_grouped\": lambda x: (\n",
    "                    x.value_counts().index[0] if not x.empty else np.nan\n",
    "                )\n",
    "            }\n",
    "            if \"event_action_grouped\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "        **({\"hit_time_2\": \"sum\"} if \"hit_time_2\" in df.columns else {}),\n",
    "        **(\n",
    "            {\n",
    "                \"hit_referer_encoded\": lambda x: (\n",
    "                    x.value_counts().index[0] if not x.empty else np.nan\n",
    "                )\n",
    "            }\n",
    "            if \"hit_referer_encoded\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "        **(\n",
    "            {\n",
    "                \"event_label_encoded\": lambda x: (\n",
    "                    x.value_counts().index[0] if not x.empty else np.nan\n",
    "                )\n",
    "            }\n",
    "            if \"event_label_encoded\" in df.columns\n",
    "            else {}\n",
    "        ),\n",
    "    }\n",
    "    utm_columns_in_hits = [\n",
    "        col for col in df.columns if col.startswith(\"utm_\") and col not in agg_rules\n",
    "    ]\n",
    "    for col in utm_columns_in_hits:\n",
    "        agg_rules[col] = lambda x: x.iloc[0] if not x.empty else np.nan\n",
    "\n",
    "    agg_rules = {\n",
    "        key: value for key, value in agg_rules.items() if key in df.columns\n",
    "    }  # Финальная проверка наличия колонок\n",
    "\n",
    "    try:\n",
    "        aggregated = df.groupby(\"session_id\").agg(agg_rules)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при агрегации: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # === Логика переименования ===\n",
    "    rename_map = {}\n",
    "    for original_col_tuple in aggregated.columns.values:\n",
    "        if isinstance(original_col_tuple, tuple):\n",
    "            original_name = original_col_tuple[0]\n",
    "            agg_name = original_col_tuple[1]\n",
    "            col_key = original_col_tuple\n",
    "            final_name = str(original_col_tuple)  # По умолчанию\n",
    "\n",
    "            if original_name == \"hit_number\":\n",
    "                if agg_name == \"min\":\n",
    "                    final_name = \"first_hit_number\"\n",
    "                elif agg_name == \"max\":\n",
    "                    final_name = \"last_hit_number\"\n",
    "                elif agg_name == \"count\":\n",
    "                    final_name = \"total_hits\"\n",
    "            elif original_name == \"hit_time_2\" and agg_name == \"sum\":\n",
    "                final_name = \"total_time\"\n",
    "            elif agg_name == \"<lambda>\":\n",
    "                if original_name == \"hit_page_path\":\n",
    "                    final_name = \"entry_page\"\n",
    "                elif original_name == \"event_category_grouped\":\n",
    "                    final_name = \"main_category_grouped\"\n",
    "                elif original_name == \"event_action_grouped\":\n",
    "                    final_name = \"main_action_grouped\"\n",
    "                elif original_name == \"hit_referer_encoded\":\n",
    "                    final_name = \"main_referer\"\n",
    "                elif original_name == \"event_label_encoded\":\n",
    "                    final_name = \"main_label\"\n",
    "                elif original_name in utm_columns_in_hits:\n",
    "                    final_name = original_name\n",
    "                else:\n",
    "                    final_name = f\"{original_name}_lambda_agg\"\n",
    "            else:\n",
    "                final_name = f\"{original_name}_{agg_name}\"\n",
    "            rename_map[col_key] = final_name\n",
    "        else:\n",
    "            rename_map[original_col_tuple] = str(original_col_tuple)  # Для SingleIndex\n",
    "\n",
    "    try:\n",
    "        aggregated.columns = [\n",
    "            rename_map.get(col, str(col)) for col in aggregated.columns\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при применении переименования столбцов: {e}\", file=sys.stderr)\n",
    "        aggregated.columns = [\n",
    "            \"_\".join(col).strip(\"_\") if isinstance(col, tuple) else str(col)\n",
    "            for col in aggregated.columns.values\n",
    "        ]\n",
    "\n",
    "    aggregated = aggregated.reset_index()\n",
    "\n",
    "    # print(\"Агрегация завершена.\", file=sys.stderr)\n",
    "    return aggregated\n",
    "\n",
    "\n",
    "def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Оптимизирует типы данных DataFrame. In-place.\"\"\"\n",
    "    # print(\"  Оптимизация типов данных...\", file=sys.stderr)\n",
    "    # initial_memory = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            if df[col].nunique() / len(df) < 0.1:\n",
    "                try:\n",
    "                    df[col] = df[col].astype(\"category\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"  Не удалось преобразовать '{col}' в категорию: {e}\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "        elif pd.api.types.is_integer_dtype(df[col]):\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Не удалось downcast '{col}' (integer): {e}\", file=sys.stderr)\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Не удалось downcast '{col}' (float): {e}\", file=sys.stderr)\n",
    "    # final_memory = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    # print(f\"  Память после оптимизации: {final_memory:.2f} MB\", file=sys.stderr)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_and_save(\n",
    "    sessions_df: pd.DataFrame, hits_df: pd.DataFrame, output_pkl: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Объединяет сессии и хиты, опционально сохраняет. Возвращает DF.\"\"\"\n",
    "    # print(\"Начало объединения данных...\", file=sys.stderr)\n",
    "    sessions_df = optimize_dtypes(sessions_df)\n",
    "    hits_df = optimize_dtypes(hits_df)\n",
    "\n",
    "    if \"session_id\" not in sessions_df.columns or \"session_id\" not in hits_df.columns:\n",
    "        print(\n",
    "            \"Ошибка: Столбец 'session_id' отсутствует в одном из датафреймов для объединения.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        merged_df = pd.merge(sessions_df, hits_df, on=\"session_id\", how=\"left\")\n",
    "        # print(\"Объединение завершено.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при объединении данных: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    merged_df = optimize_dtypes(merged_df)\n",
    "\n",
    "    if output_pkl:\n",
    "        # print(f\"Сохранение данных в {output_pkl}...\", file=sys.stderr)\n",
    "        try:\n",
    "            with open(output_pkl, \"wb\") as f:\n",
    "                pickle.dump(merged_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            # pkl_size = Path(output_pkl).stat().st_size / (1024 * 1024)\n",
    "            # print(f\"Файл сохранен. Размер: {pkl_size:.2f} MB\", file=sys.stderr)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при сохранении файла {output_pkl}: {e}\", file=sys.stderr)\n",
    "\n",
    "    # print(\"Объединение и сохранение завершены.\", file=sys.stderr)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def reduce_cardinality(\n",
    "    df: pd.DataFrame, categorical_features: list, max_categories: int = 50\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Уменьшает кардинальность категориальных признаков.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns and df[col].dtype == \"object\":\n",
    "            if df[col].nunique() > max_categories:\n",
    "                top_categories = (\n",
    "                    df[col].value_counts().nlargest(max_categories - 1).index\n",
    "                )\n",
    "                if \"OTHER\" in top_categories:\n",
    "                    top_categories = (\n",
    "                        df[col].value_counts().nlargest(max_categories).index\n",
    "                    )\n",
    "                    if \"OTHER\" in top_categories:\n",
    "                        top_categories = top_categories.drop(\"OTHER\")[\n",
    "                            : max_categories - 1\n",
    "                        ]\n",
    "                df[col] = np.where(df[col].isin(top_categories), df[col], \"OTHER\")\n",
    "                try:\n",
    "                    df[col] = df[col].astype(\"category\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Не удалось преобразовать '{col}' в категорию после reduce_cardinality: {e}\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "            elif df[col].nunique() > 0:\n",
    "                try:\n",
    "                    df[col] = df[col].astype(\"category\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Не удалось преобразовать '{col}' в категорию: {e}\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_rare_classes(y: pd.Series, min_samples: int = 2) -> pd.Index:\n",
    "    \"\"\"Возвращает индекс классов с достаточным количеством образцов.\"\"\"\n",
    "    if not isinstance(y, pd.Series) or y.empty:\n",
    "        return pd.Index([])\n",
    "    value_counts = y.value_counts()\n",
    "    valid_classes = value_counts[value_counts >= min_samples].index\n",
    "    if pd.isna(valid_classes).any():\n",
    "        valid_classes = valid_classes.dropna()\n",
    "    return valid_classes\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Определение признаков для модели (Скопированы из полного пайплайна)\n",
    "#    Примечание: Эти списки должны точно соответствовать тем, что использовались при обучении.\n",
    "# ==============================================================================\n",
    "\n",
    "numeric_features = [\n",
    "    \"visit_number\",\n",
    "    \"day_of_week_num\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"utm_source_encoded\",\n",
    "    \"utm_campaign_encoded\",\n",
    "    \"first_hit_number\",\n",
    "    \"last_hit_number\",\n",
    "    \"total_hits\",\n",
    "    \"total_time\",\n",
    "    \"main_referer\",\n",
    "    \"main_label\",\n",
    "    \"geo_country\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"utm_medium\",\n",
    "    \"device_category\",\n",
    "    \"device_brand\",\n",
    "    \"geo_city_grouped\",\n",
    "    \"entry_page\",\n",
    "    \"main_category_grouped\",\n",
    "]\n",
    "\n",
    "# Имя целевого столбца (не признак)\n",
    "target_column_name_for_prediction = \"main_action_grouped\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Функция предобработки данных (Скопирована из полного пайплайна)\n",
    "#    Инкапсулирует весь пайплайн предобработки данных.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "    ga_sessions_df_raw: pd.DataFrame, ga_hits_df_raw: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Выполняет полную предобработку данных сессий и хитов, объединяет их\n",
    "    и возвращает датафрейм, готовый для обучения или предсказания.\n",
    "    Не включает начальную загрузку файлов и process_notset_none,\n",
    "    а также финальное обучение/предсказание.\n",
    "    \"\"\"\n",
    "    # print(\"\\n--- Запуск функции preprocess_data ---\", file=sys.stderr)\n",
    "\n",
    "    # Создаем копии для работы внутри функции\n",
    "    ga_sessions_filled = ga_sessions_df_raw.copy()\n",
    "    ga_hits_filled = ga_hits_df_raw.copy()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Предобработка данных сессий (ga_sessions_filled)\n",
    "    # --------------------------------------------------------------------------\n",
    "    # print(\"-> Предобработка данных сессий...\", file=sys.stderr)\n",
    "    if \"visit_date\" in ga_sessions_filled.columns:\n",
    "        try:\n",
    "            ga_sessions_filled[\"visit_date\"] = pd.to_datetime(\n",
    "                ga_sessions_filled[\"visit_date\"], errors=\"coerce\"\n",
    "            )  # errors='coerce' заменит некорректные даты на NaT\n",
    "            ga_sessions_filled.dropna(\n",
    "                subset=[\"visit_date\"], inplace=True\n",
    "            )  # Удалим строки с некорректными датами\n",
    "            if not ga_sessions_filled.empty:\n",
    "                ga_sessions_filled[\"year\"] = ga_sessions_filled[\n",
    "                    \"visit_date\"\n",
    "                ].dt.year.astype(\"int16\")\n",
    "                ga_sessions_filled[\"month\"] = ga_sessions_filled[\n",
    "                    \"visit_date\"\n",
    "                ].dt.month.astype(\"int16\")\n",
    "                ga_sessions_filled[\"day\"] = ga_sessions_filled[\n",
    "                    \"visit_date\"\n",
    "                ].dt.day.astype(\"int16\")\n",
    "                ga_sessions_filled[\"day_of_week_num\"] = ga_sessions_filled[\n",
    "                    \"visit_date\"\n",
    "                ].dt.dayofweek.astype(\"int16\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при создании признаков даты: {e}\", file=sys.stderr)\n",
    "\n",
    "    ga_sessions_filled = fill_missing_by_groups(ga_sessions_filled)\n",
    "\n",
    "    if \"utm_source\" in ga_sessions_filled.columns:\n",
    "        ga_sessions_filled = encode_column_with_nulls(ga_sessions_filled, \"utm_source\")\n",
    "    if \"utm_campaign\" in ga_sessions_filled.columns:\n",
    "        ga_sessions_filled = encode_column_with_nulls(\n",
    "            ga_sessions_filled, \"utm_campaign\"\n",
    "        )\n",
    "    if \"device_model\" in ga_sessions_filled.columns:\n",
    "        ga_sessions_filled = encode_column_with_nulls(\n",
    "            ga_sessions_filled, \"device_model\"\n",
    "        )\n",
    "\n",
    "    if \"geo_country\" in ga_sessions_filled.columns:\n",
    "        ga_sessions_filled[\"geo_country\"] = (\n",
    "            ga_sessions_filled[\"geo_country\"]\n",
    "            .astype(str)\n",
    "            .apply(lambda x: 1 if x == \"Russia\" else 0)\n",
    "        )\n",
    "\n",
    "    if \"geo_city\" in ga_sessions_filled.columns and not ga_sessions_filled.empty:\n",
    "        threshold = len(ga_sessions_filled) * 0.01\n",
    "        value_counts = ga_sessions_filled[\"geo_city\"].value_counts()\n",
    "        replace_dict = {\n",
    "            category: \"OTHER\"\n",
    "            for category in value_counts[value_counts < threshold].index\n",
    "        }\n",
    "        ga_sessions_filled[\"geo_city_grouped\"] = ga_sessions_filled[\"geo_city\"].replace(\n",
    "            replace_dict\n",
    "        )\n",
    "    elif \"geo_city\" in ga_sessions_filled.columns:  # Handle empty df but column exists\n",
    "        ga_sessions_filled[\"geo_city_grouped\"] = \"OTHER\"\n",
    "\n",
    "    # print(\"-> Предобработка данных сессий завершена.\", file=sys.stderr)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Предобработка и агрегация данных хитов (ga_hits_aggregated)\n",
    "    # --------------------------------------------------------------------------\n",
    "    # print(\"\\n-> Предобработка и агрегация данных хитов...\", file=sys.stderr)\n",
    "    ga_hits_filled = fill_hits_and_events(ga_hits_filled)\n",
    "\n",
    "    if \"hit_time\" in ga_hits_filled.columns:\n",
    "        ga_hits_filled[\"hit_time_2\"] = ga_hits_filled[\"hit_time\"].notna().astype(int)\n",
    "\n",
    "    if \"hit_referer\" in ga_hits_filled.columns:\n",
    "        ga_hits_filled = encode_column_with_nulls(ga_hits_filled, \"hit_referer\")\n",
    "\n",
    "    if \"event_category\" in ga_hits_filled.columns and not ga_hits_filled.empty:\n",
    "        threshold = len(ga_hits_filled) * 0.01\n",
    "        value_counts = ga_hits_filled[\"event_category\"].value_counts()\n",
    "        replace_dict = {\n",
    "            category: \"OTHER\"\n",
    "            for category in value_counts[value_counts < threshold].index\n",
    "        }\n",
    "        ga_hits_filled[\"event_category_grouped\"] = ga_hits_filled[\n",
    "            \"event_category\"\n",
    "        ].replace(replace_dict)\n",
    "    elif (\n",
    "        \"event_category\" in ga_hits_filled.columns\n",
    "    ):  # Handle empty df but column exists\n",
    "        ga_hits_filled[\"event_category_grouped\"] = \"OTHER\"\n",
    "\n",
    "    if \"event_action\" in ga_hits_filled.columns and not ga_hits_filled.empty:\n",
    "        threshold = len(ga_hits_filled) * 0.01\n",
    "        value_counts = ga_hits_filled[\"event_action\"].value_counts()\n",
    "        replace_dict = {\n",
    "            category: \"OTHER\"\n",
    "            for category in value_counts[value_counts < threshold].index\n",
    "        }\n",
    "        ga_hits_filled[\"event_action_grouped\"] = ga_hits_filled[\"event_action\"].replace(\n",
    "            replace_dict\n",
    "        )\n",
    "    elif \"event_action\" in ga_hits_filled.columns:  # Handle empty df but column exists\n",
    "        ga_hits_filled[\"event_action_grouped\"] = \"OTHER\"\n",
    "\n",
    "    if \"event_label\" in ga_hits_filled.columns:\n",
    "        ga_hits_filled = encode_column_with_nulls(ga_hits_filled, \"event_label\")\n",
    "\n",
    "    columns_to_drop_from_hits_filled = [\n",
    "        \"event_value\",\n",
    "        \"hit_date\",\n",
    "        \"hit_type\",\n",
    "        \"hit_time\",\n",
    "        \"hit_referer\",\n",
    "        \"event_label\",\n",
    "    ]\n",
    "    existing_cols_to_drop_hits = [\n",
    "        col for col in columns_to_drop_from_hits_filled if col in ga_hits_filled.columns\n",
    "    ]\n",
    "    if existing_cols_to_drop_hits:\n",
    "        ga_hits_filled = ga_hits_filled.drop(columns=existing_cols_to_drop_hits, axis=1)\n",
    "\n",
    "    columns_to_drop_from_sessions_filled = [\n",
    "        \"visit_date\",\n",
    "        \"visit_time\",\n",
    "        \"utm_source\",\n",
    "        \"utm_campaign\",\n",
    "        \"utm_adcontent\",\n",
    "        \"utm_keyword\",\n",
    "        \"device_screen_resolution\",\n",
    "        \"device_os\",\n",
    "        \"geo_city\",\n",
    "        \"year\",\n",
    "    ]\n",
    "    existing_cols_to_drop_sessions = [\n",
    "        col\n",
    "        for col in columns_to_drop_from_sessions_filled\n",
    "        if col in ga_sessions_filled.columns\n",
    "    ]\n",
    "    if existing_cols_to_drop_sessions:\n",
    "        ga_sessions_filled = ga_sessions_filled.drop(\n",
    "            columns=existing_cols_to_drop_sessions, axis=1\n",
    "        )\n",
    "\n",
    "    ga_hits_aggregated = aggregate_session_data(ga_hits_filled)\n",
    "    del ga_hits_filled\n",
    "    gc.collect()\n",
    "    if ga_hits_aggregated.empty:\n",
    "        print(\n",
    "            \"Предупреждение: ga_hits_aggregated пуст после агрегации.\", file=sys.stderr\n",
    "        )\n",
    "\n",
    "    columns_to_drop_from_hits_aggregated = [\"main_category\", \"main_action\"]\n",
    "    existing_cols_to_drop_hits_agg = [\n",
    "        col\n",
    "        for col in columns_to_drop_from_hits_aggregated\n",
    "        if col in ga_hits_aggregated.columns\n",
    "    ]\n",
    "    if existing_cols_to_drop_hits_agg:\n",
    "        ga_hits_aggregated = ga_hits_aggregated.drop(\n",
    "            columns=existing_cols_to_drop_hits_agg, axis=1\n",
    "        )\n",
    "\n",
    "    # print(\"-> Предобработка и агрегация данных хитов завершена.\", file=sys.stderr)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Объединение датафреймов\n",
    "    # --------------------------------------------------------------------------\n",
    "    # print(\"\\n-> Объединение датафреймов...\", file=sys.stderr)\n",
    "    # Не сохраняем промежуточные файлы merged_data.pkl / merged_data_2.pkl в функции предобработки\n",
    "    # Это опционально и должно быть вне этой функции, если нужно для отладки или хранения\n",
    "    data = merge_and_save(\n",
    "        ga_sessions_filled, ga_hits_aggregated, output_pkl=None\n",
    "    )  # Убрал сохранение\n",
    "    del ga_sessions_filled, ga_hits_aggregated\n",
    "    gc.collect()\n",
    "\n",
    "    if data.empty:\n",
    "        print(\"Ошибка: Объединенный датафрейм пуст после слияния.\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    columns_to_drop_from_data = [\n",
    "        \"device_model\",\n",
    "        \"device_browser\",\n",
    "        \"device_model_encoded\",\n",
    "    ]\n",
    "    existing_cols_to_drop_data = [\n",
    "        col for col in columns_to_drop_from_data if col in data.columns\n",
    "    ]\n",
    "    if existing_cols_to_drop_data:\n",
    "        data = data.drop(columns=existing_cols_to_drop_data, axis=1)\n",
    "\n",
    "    # Удаляем действия без пользователей (строки с NaN в 'first_hit_number')\n",
    "    if \"first_hit_number\" in data.columns:\n",
    "        data.dropna(subset=[\"first_hit_number\"], inplace=True)\n",
    "    # else:\n",
    "    # print(\"'first_hit_number' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    if data.empty:\n",
    "        print(\n",
    "            \"Ошибка: Датафрейм пуст после финальной очистки (dropna).\", file=sys.stderr\n",
    "        )\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # print(\"\\n--- Функция preprocess_data завершена ---\", file=sys.stderr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Основной исполняемый код: Замер времени предсказания\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Запуск скрипта для замера времени предсказания.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"==============================================================================\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # Настройка путей к валидационным данным и обученной модели\n",
    "    file_path_hits_valid = \"ga_hits_valid.pkl\"  # Файл с валидационными хитами\n",
    "    file_path_sessions_valid = \"ga_sessions_valid.pkl\"  # Файл с валидационными сессиями\n",
    "    model_filepath = \"lgbm_model.pkl\"  # Файл с обученной моделью\n",
    "\n",
    "    # Убедимся, что файлы существуют\n",
    "    if not Path(file_path_hits_valid).exists():\n",
    "        print(\n",
    "            f\"Ошибка: Валидационный файл хитов '{file_path_hits_valid}' не найден. Замер невозможен.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    if not Path(file_path_sessions_valid).exists():\n",
    "        print(\n",
    "            f\"Ошибка: Валидационный файл сессий '{file_path_sessions_valid}' не найден. Замер невозможен.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        sys.exit(1)\n",
    "    if not Path(model_filepath).exists():\n",
    "        print(\n",
    "            f\"Ошибка: Файл обученной модели '{model_filepath}' не найден. Замер невозможен.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Начало замера времени ---\n",
    "    start_time = time.time()\n",
    "    print(\"\\n--- Начало замера времени ---\", file=sys.stderr)\n",
    "\n",
    "    # 1) Прочитать датафреймы\n",
    "    print(\"Загрузка валидационных данных...\", file=sys.stderr)\n",
    "    sessions_valid_raw = None\n",
    "    hits_valid_raw = None\n",
    "    try:\n",
    "        sessions_valid_raw = pd.read_pickle(file_path_sessions_valid)\n",
    "        print(\n",
    "            f\"Файл '{file_path_sessions_valid}' успешно загружен. Размер: {len(sessions_valid_raw)} строк.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке '{file_path_sessions_valid}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        hits_valid_raw = pd.read_pickle(file_path_hits_valid)\n",
    "        print(\n",
    "            f\"Файл '{file_path_hits_valid}' успешно загружен. Размер: {len(hits_valid_raw)} строк.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке '{file_path_hits_valid}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if sessions_valid_raw.empty or hits_valid_raw.empty:\n",
    "        print(\n",
    "            \"Ошибка: Загруженные валидационные датафреймы пусты. Замер невозможен.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Применяем начальную предобработку 'not set'/'none'\n",
    "    print(\n",
    "        \"Применение начальной предобработки 'not set'/'none' к валидационным данным...\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    process_notset_none(sessions_valid_raw)\n",
    "    process_notset_none(hits_valid_raw)\n",
    "\n",
    "    # 2) Выполнить весь пиплайн предобработки\n",
    "    print(\n",
    "        \"\\nВыполнение полного пайплайна предобработки на валидационных данных...\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    data_valid_preprocessed = preprocess_data(\n",
    "        sessions_valid_raw.copy(), hits_valid_raw.copy()\n",
    "    )\n",
    "\n",
    "    # Освобождаем память от исходных валидационных данных\n",
    "    del sessions_valid_raw, hits_valid_raw\n",
    "    gc.collect()\n",
    "\n",
    "    if data_valid_preprocessed.empty:\n",
    "        print(\n",
    "            \"Ошибка: Данные после предобработки пусты. Предсказание невозможно.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3) Загрузить обученную модель\n",
    "    print(f\"\\nЗагрузка обученной модели из '{model_filepath}'...\", file=sys.stderr)\n",
    "    model_pipeline = None\n",
    "    try:\n",
    "        with open(model_filepath, \"rb\") as f:\n",
    "            model_pipeline = pickle.load(f)\n",
    "        print(\"Модель успешно загружена.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке модели: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Подготовка данных для предсказания (удаление целевого столбца, если есть, и session_id)\n",
    "    # preprocess_data не удаляет целевой столбец и session_id\n",
    "    print(\"Подготовка данных для предсказания...\", file=sys.stderr)\n",
    "    X_valid = data_valid_preprocessed.drop(\n",
    "        columns=[target_column_name_for_prediction, \"session_id\"], errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    if X_valid.empty:\n",
    "        print(\n",
    "            \"Ошибка: Нет признаков для предсказания после подготовки данных. Замер невозможен.\",\n",
    "            file=sys.stderr,\n",
    "        )\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 4) Выдача предикта\n",
    "    print(\"\\nВыполнение предсказания...\", file=sys.stderr)\n",
    "    predictions = None\n",
    "    try:\n",
    "        # Используйте .predict() для получения предсказанных классов\n",
    "        predictions = model_pipeline.predict(X_valid)\n",
    "        print(\"Предсказание выполнено.\", file=sys.stderr)\n",
    "\n",
    "        # Если нужны вероятности, используйте .predict_proba()\n",
    "        # probabilities = model_pipeline.predict_proba(X_valid)\n",
    "        # print(\"Предсказанные вероятности получены.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при выполнении предсказания: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Конец замера времени ---\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\"\\n--- Конец замера времени ---\", file=sys.stderr)\n",
    "    print(\n",
    "        f\"Общее время выполнения пайплайна (загрузка валидации -> предобработка -> загрузка модели -> предсказание): {elapsed_time:.4f} секунд\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "    # Вывод первых нескольких предсказаний для проверки\n",
    "    print(\"\\nПервые 10 предсказанных значений:\", file=sys.stderr)\n",
    "    if predictions is not None:\n",
    "        print(predictions[:10], file=sys.stderr)\n",
    "    else:\n",
    "        print(\"Предсказания не были получены.\", file=sys.stderr)\n",
    "\n",
    "    print(\n",
    "        \"\\n==============================================================================\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "    print(\"Скрипт замера времени предсказания завершен.\", file=sys.stderr)\n",
    "    print(\n",
    "        \"==============================================================================\",\n",
    "        file=sys.stderr,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b1a303-a4a1-4fe3-b265-ce8b6d21eb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c40aa-9e6a-46df-a469-66f7f249671a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21e4f7a-681e-4f07-b98e-68e07ef35e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

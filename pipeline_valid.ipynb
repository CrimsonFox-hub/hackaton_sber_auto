{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4a175-935e-4dbe-92ab-d04066959be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скрипт для выполнения предсказания на валидационных данных\n",
    "# с замером времени.\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. Импорт необходимых библиотек\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time # Для замера времени\n",
    "import sys\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# Импорты sklearn (необходимы для функций предобработки и загрузки модели)\n",
    "from sklearn.model_selection import train_test_split # Хотя train_test_split не используется в предсказании, он часть preprocess_data\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score # Не используется для предсказания, но может быть в загруженной модели\n",
    "\n",
    "# Импорт LGBMClassifier (необходим для загрузки модели)\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Определение вспомогательных функций (Скопированы из полного пайплайна)\n",
    "#    Примечание: Эти функции нужны для работы preprocess_data.\n",
    "# ==============================================================================\n",
    "\n",
    "def process_notset_none(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Обнаруживает строки 'not set' и 'none' и заменяет на np.nan. In-place.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame): return # Добавлена проверка\n",
    "    strings_to_find_and_replace = ['not set', 'none']\n",
    "    # print(\"Начало обработки 'not set' и 'none'\", file=sys.stderr)\n",
    "    for col in df.columns:\n",
    "        col_series_str = df[col].astype(str)\n",
    "        combined_mask = col_series_str.str.contains(strings_to_find_and_replace[0], case=False, regex=False) | \\\n",
    "                        col_series_str.str.contains(strings_to_find_and_replace[1], case=False, regex=False)\n",
    "        if combined_mask.sum() > 0:\n",
    "            df.loc[combined_mask, col] = np.nan\n",
    "    # print(\"Обработка 'not set' и 'none' завершена.\", file=sys.stderr)\n",
    "\n",
    "def encode_column_with_nulls(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Кодирует столбец числовыми метками (NaN->0, остальное 1+). Возвращает новый DF.\"\"\"\n",
    "    df = df.copy()\n",
    "    if column_name not in df.columns: return df\n",
    "    encoded, uniques = pd.factorize(df[column_name].astype(str))\n",
    "    df[f'{column_name}_encoded'] = np.where(encoded == -1, 0, encoded + 1)\n",
    "    df[f'{column_name}_encoded'] = df[f'{column_name}_encoded'].astype('int64')\n",
    "    return df\n",
    "\n",
    "def get_mode(x):\n",
    "    \"\"\"Вспомогательная функция для моды.\"\"\"\n",
    "    mode_val = x.mode()\n",
    "    if not mode_val.empty: return mode_val[0]\n",
    "    return np.nan\n",
    "\n",
    "def fill_missing_by_groups(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Заполняет пропуски по группам и модой. In-place.\"\"\"\n",
    "    # print(\"Начало заполнения пропусков по группам...\", file=sys.stderr)\n",
    "    if 'utm_source' in df.columns and 'utm_medium' in df.columns and 'device_category' in df.columns and df['utm_source'].isna().any():\n",
    "        group_modes_source = df.groupby(['utm_medium', 'device_category'])['utm_source'].transform(get_mode)\n",
    "        df['utm_source'].fillna(group_modes_source, inplace=True)\n",
    "    if 'utm_campaign' in df.columns and 'utm_medium' in df.columns and 'device_os' in df.columns and df['utm_campaign'].isna().any():\n",
    "         group_modes_campaign = df.groupby(['utm_medium', 'device_os'])['utm_campaign'].transform(get_mode)\n",
    "         df['utm_campaign'].fillna(group_modes_campaign, inplace=True)\n",
    "\n",
    "    group_columns_broad = ['utm_medium', 'device_category', 'device_os']\n",
    "    if all(col in df.columns for col in group_columns_broad):\n",
    "        cols_with_nan_after_specific = df.columns[df.isna().any()].tolist()\n",
    "        for col in cols_with_nan_after_specific:\n",
    "             if col in ['utm_source', 'utm_campaign'] and not df[col].isna().any(): continue\n",
    "             group_modes_broad_col = df.groupby(group_columns_broad)[col].transform(get_mode)\n",
    "             df[col].fillna(group_modes_broad_col, inplace=True)\n",
    "\n",
    "    cols_with_nan_final = df.columns[df.isna().any()].tolist()\n",
    "    for col in cols_with_nan_final:\n",
    "        mode_val = df[col].mode()\n",
    "        if not mode_val.empty: df[col].fillna(mode_val[0], inplace=True)\n",
    "    # print(\"Заполнение пропусков завершено.\", file=sys.stderr)\n",
    "    return df\n",
    "\n",
    "def fill_hits_and_events(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Заполняет пропуски в hit_referer и event_label. In-place.\"\"\"\n",
    "    # print(\"Начало заполнения пропусков в hits...\", file=sys.stderr)\n",
    "    if 'hit_referer' in df.columns and 'hit_page_path' in df.columns and df['hit_referer'].isna().any():\n",
    "        referer_by_page = df.groupby('hit_page_path')['hit_referer'].transform('first')\n",
    "        df['hit_referer'].fillna(referer_by_page, inplace=True)\n",
    "    if 'event_label' in df.columns and 'event_action' in df.columns and df['event_label'].isna().any():\n",
    "        label_by_action = df.groupby('event_action')['event_label'].transform('first')\n",
    "        df['event_label'].fillna(label_by_action, inplace=True)\n",
    "    if 'hit_referer' in df.columns and df['hit_referer'].isna().any():\n",
    "        df['hit_referer'].fillna('direct', inplace=True)\n",
    "    if 'event_label' in df.columns and df['event_label'].isna().any():\n",
    "        df['event_label'].fillna('none', inplace=True)\n",
    "    # print(\"Заполнение пропусков в hits завершено.\", file=sys.stderr)\n",
    "    return df\n",
    "\n",
    "def aggregate_session_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Агрегирует данные хитов до уровня сессий.\"\"\"\n",
    "    # print(\"Начало агрегации данных хитов...\", file=sys.stderr)\n",
    "    if 'session_id' not in df.columns:\n",
    "         print(\"Ошибка: 'session_id' отсутствует для агрегации.\", file=sys.stderr)\n",
    "         return pd.DataFrame()\n",
    "\n",
    "    agg_rules = {\n",
    "        'hit_number': ['min', 'max', 'count'],\n",
    "        **({'hit_page_path': lambda x: x.iloc[0] if not x.empty else np.nan} if 'hit_page_path' in df.columns else {}),\n",
    "        **({'event_category_grouped': lambda x: x.value_counts().index[0] if not x.empty else np.nan} if 'event_category_grouped' in df.columns else {}),\n",
    "        **({'event_action_grouped': lambda x: x.value_counts().index[0] if not x.empty else np.nan} if 'event_action_grouped' in df.columns else {}),\n",
    "        **({'hit_time_2': 'sum'} if 'hit_time_2' in df.columns else {}),\n",
    "        **({'hit_referer_encoded': lambda x: x.value_counts().index[0] if not x.empty else np.nan} if 'hit_referer_encoded' in df.columns else {}),\n",
    "        **({'event_label_encoded': lambda x: x.value_counts().index[0] if not x.empty else np.nan} if 'event_label_encoded' in df.columns else {}),\n",
    "    }\n",
    "    utm_columns_in_hits = [col for col in df.columns if col.startswith('utm_') and col not in agg_rules]\n",
    "    for col in utm_columns_in_hits:\n",
    "         agg_rules[col] = lambda x: x.iloc[0] if not x.empty else np.nan\n",
    "\n",
    "    agg_rules = {key: value for key, value in agg_rules.items() if key in df.columns} # Финальная проверка наличия колонок\n",
    "\n",
    "\n",
    "    try:\n",
    "        aggregated = df.groupby('session_id').agg(agg_rules)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при агрегации: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # === Логика переименования ===\n",
    "    rename_map = {}\n",
    "    for original_col_tuple in aggregated.columns.values:\n",
    "        if isinstance(original_col_tuple, tuple):\n",
    "            original_name = original_col_tuple[0]\n",
    "            agg_name = original_col_tuple[1]\n",
    "            col_key = original_col_tuple\n",
    "            final_name = str(original_col_tuple) # По умолчанию\n",
    "\n",
    "            if original_name == 'hit_number':\n",
    "                if agg_name == 'min': final_name = 'first_hit_number'\n",
    "                elif agg_name == 'max': final_name = 'last_hit_number'\n",
    "                elif agg_name == 'count': final_name = 'total_hits'\n",
    "            elif original_name == 'hit_time_2' and agg_name == 'sum': final_name = 'total_time'\n",
    "            elif agg_name == '<lambda>':\n",
    "                 if original_name == 'hit_page_path': final_name = 'entry_page'\n",
    "                 elif original_name == 'event_category_grouped': final_name = 'main_category_grouped'\n",
    "                 elif original_name == 'event_action_grouped': final_name = 'main_action_grouped'\n",
    "                 elif original_name == 'hit_referer_encoded': final_name = 'main_referer'\n",
    "                 elif original_name == 'event_label_encoded': final_name = 'main_label'\n",
    "                 elif original_name in utm_columns_in_hits: final_name = original_name\n",
    "                 else: final_name = f\"{original_name}_lambda_agg\"\n",
    "            else: final_name = f\"{original_name}_{agg_name}\"\n",
    "            rename_map[col_key] = final_name\n",
    "        else:\n",
    "            rename_map[original_col_tuple] = str(original_col_tuple) # Для SingleIndex\n",
    "\n",
    "\n",
    "    try:\n",
    "        aggregated.columns = [rename_map.get(col, str(col)) for col in aggregated.columns]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при применении переименования столбцов: {e}\", file=sys.stderr)\n",
    "        aggregated.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else str(col) for col in aggregated.columns.values]\n",
    "\n",
    "\n",
    "    aggregated = aggregated.reset_index()\n",
    "\n",
    "    # print(\"Агрегация завершена.\", file=sys.stderr)\n",
    "    return aggregated\n",
    "\n",
    "def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Оптимизирует типы данных DataFrame. In-place.\"\"\"\n",
    "    # print(\"  Оптимизация типов данных...\", file=sys.stderr)\n",
    "    # initial_memory = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            if df[col].nunique() / len(df) < 0.1:\n",
    "                 try: df[col] = df[col].astype('category')\n",
    "                 except Exception as e: print(f\"  Не удалось преобразовать '{col}' в категорию: {e}\", file=sys.stderr)\n",
    "        elif pd.api.types.is_integer_dtype(df[col]):\n",
    "             try: df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "             except Exception as e: print(f\"  Не удалось downcast '{col}' (integer): {e}\", file=sys.stderr)\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "             try: df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "             except Exception as e: print(f\"  Не удалось downcast '{col}' (float): {e}\", file=sys.stderr)\n",
    "    # final_memory = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    # print(f\"  Память после оптимизации: {final_memory:.2f} MB\", file=sys.stderr)\n",
    "    return df\n",
    "\n",
    "def merge_and_save(sessions_df: pd.DataFrame, hits_df: pd.DataFrame, output_pkl: str = None) -> pd.DataFrame:\n",
    "    \"\"\"Объединяет сессии и хиты, опционально сохраняет. Возвращает DF.\"\"\"\n",
    "    # print(\"Начало объединения данных...\", file=sys.stderr)\n",
    "    sessions_df = optimize_dtypes(sessions_df)\n",
    "    hits_df = optimize_dtypes(hits_df)\n",
    "\n",
    "    if 'session_id' not in sessions_df.columns or 'session_id' not in hits_df.columns:\n",
    "        print(\"Ошибка: Столбец 'session_id' отсутствует в одном из датафреймов для объединения.\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        merged_df = pd.merge(sessions_df, hits_df, on='session_id', how='left')\n",
    "        # print(\"Объединение завершено.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при объединении данных: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    merged_df = optimize_dtypes(merged_df)\n",
    "\n",
    "    if output_pkl:\n",
    "        # print(f\"Сохранение данных в {output_pkl}...\", file=sys.stderr)\n",
    "        try:\n",
    "            with open(output_pkl, 'wb') as f:\n",
    "                pickle.dump(merged_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            # pkl_size = Path(output_pkl).stat().st_size / (1024 * 1024)\n",
    "            # print(f\"Файл сохранен. Размер: {pkl_size:.2f} MB\", file=sys.stderr)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при сохранении файла {output_pkl}: {e}\", file=sys.stderr)\n",
    "\n",
    "    # print(\"Объединение и сохранение завершены.\", file=sys.stderr)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def reduce_cardinality(df: pd.DataFrame, categorical_features: list, max_categories: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"Уменьшает кардинальность категориальных признаков.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "             if df[col].nunique() > max_categories:\n",
    "                top_categories = df[col].value_counts().nlargest(max_categories-1).index\n",
    "                if 'OTHER' in top_categories:\n",
    "                     top_categories = df[col].value_counts().nlargest(max_categories).index\n",
    "                     if 'OTHER' in top_categories: top_categories = top_categories.drop('OTHER')[:max_categories-1]\n",
    "                df[col] = np.where(df[col].isin(top_categories), df[col], 'OTHER')\n",
    "                try: df[col] = df[col].astype('category')\n",
    "                except Exception as e: print(f\"Не удалось преобразовать '{col}' в категорию после reduce_cardinality: {e}\", file=sys.stderr)\n",
    "             elif df[col].nunique() > 0:\n",
    "                 try: df[col] = df[col].astype('category')\n",
    "                 except Exception as e: print(f\"Не удалось преобразовать '{col}' в категорию: {e}\", file=sys.stderr)\n",
    "    return df\n",
    "\n",
    "def filter_rare_classes(y: pd.Series, min_samples: int = 2) -> pd.Index:\n",
    "    \"\"\"Возвращает индекс классов с достаточным количеством образцов.\"\"\"\n",
    "    if not isinstance(y, pd.Series) or y.empty: return pd.Index([])\n",
    "    value_counts = y.value_counts()\n",
    "    valid_classes = value_counts[value_counts >= min_samples].index\n",
    "    if pd.isna(valid_classes).any(): valid_classes = valid_classes.dropna()\n",
    "    return valid_classes\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Определение признаков для модели (Скопированы из полного пайплайна)\n",
    "#    Примечание: Эти списки должны точно соответствовать тем, что использовались при обучении.\n",
    "# ==============================================================================\n",
    "\n",
    "numeric_features = [\n",
    "    'visit_number', 'day_of_week_num', 'month', 'day',\n",
    "    'utm_source_encoded', 'utm_campaign_encoded',\n",
    "    'first_hit_number', 'last_hit_number', 'total_hits', 'total_time',\n",
    "    'main_referer', 'main_label', 'geo_country'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'utm_medium', 'device_category', 'device_brand',\n",
    "    'geo_city_grouped', 'entry_page', 'main_category_grouped',\n",
    "]\n",
    "\n",
    "# Имя целевого столбца (не признак)\n",
    "target_column_name_for_prediction = 'main_action_grouped'\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Функция предобработки данных (Скопирована из полного пайплайна)\n",
    "#    Инкапсулирует весь пайплайн предобработки данных.\n",
    "# ==============================================================================\n",
    "\n",
    "def preprocess_data(ga_sessions_df_raw: pd.DataFrame, ga_hits_df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Выполняет полную предобработку данных сессий и хитов, объединяет их\n",
    "    и возвращает датафрейм, готовый для обучения или предсказания.\n",
    "    Не включает начальную загрузку файлов и process_notset_none,\n",
    "    а также финальное обучение/предсказание.\n",
    "    \"\"\"\n",
    "    # print(\"\\n--- Запуск функции preprocess_data ---\", file=sys.stderr)\n",
    "\n",
    "    # Создаем копии для работы внутри функции\n",
    "    ga_sessions_filled = ga_sessions_df_raw.copy()\n",
    "    ga_hits_filled = ga_hits_df_raw.copy()\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Предобработка данных сессий (ga_sessions_filled)\n",
    "    # --------------------------------------------------------------------------\n",
    "    # print(\"-> Предобработка данных сессий...\", file=sys.stderr)\n",
    "    if 'visit_date' in ga_sessions_filled.columns:\n",
    "        try:\n",
    "            ga_sessions_filled['visit_date'] = pd.to_datetime(ga_sessions_filled['visit_date'], errors='coerce') # errors='coerce' заменит некорректные даты на NaT\n",
    "            ga_sessions_filled.dropna(subset=['visit_date'], inplace=True) # Удалим строки с некорректными датами\n",
    "            if not ga_sessions_filled.empty:\n",
    "                ga_sessions_filled['year'] = ga_sessions_filled['visit_date'].dt.year.astype('int16')\n",
    "                ga_sessions_filled['month'] = ga_sessions_filled['visit_date'].dt.month.astype('int16')\n",
    "                ga_sessions_filled['day'] = ga_sessions_filled['visit_date'].dt.day.astype('int16')\n",
    "                ga_sessions_filled['day_of_week_num'] = ga_sessions_filled['visit_date'].dt.dayofweek.astype('int16')\n",
    "        except Exception as e: print(f\"Ошибка при создании признаков даты: {e}\", file=sys.stderr)\n",
    "\n",
    "    ga_sessions_filled = fill_missing_by_groups(ga_sessions_filled)\n",
    "\n",
    "    if 'utm_source' in ga_sessions_filled.columns: ga_sessions_filled = encode_column_with_nulls(ga_sessions_filled, 'utm_source')\n",
    "    if 'utm_campaign' in ga_sessions_filled.columns: ga_sessions_filled = encode_column_with_nulls(ga_sessions_filled, 'utm_campaign')\n",
    "    if 'device_model' in ga_sessions_filled.columns: ga_sessions_filled = encode_column_with_nulls(ga_sessions_filled, 'device_model')\n",
    "\n",
    "    if 'geo_country' in ga_sessions_filled.columns:\n",
    "        ga_sessions_filled['geo_country'] = ga_sessions_filled['geo_country'].astype(str).apply(lambda x: 1 if x == 'Russia' else 0)\n",
    "\n",
    "    if 'geo_city' in ga_sessions_filled.columns and not ga_sessions_filled.empty:\n",
    "        threshold = len(ga_sessions_filled) * 0.01\n",
    "        value_counts = ga_sessions_filled['geo_city'].value_counts()\n",
    "        replace_dict = {category: 'OTHER' for category in value_counts[value_counts < threshold].index}\n",
    "        ga_sessions_filled['geo_city_grouped'] = ga_sessions_filled['geo_city'].replace(replace_dict)\n",
    "    elif 'geo_city' in ga_sessions_filled.columns: # Handle empty df but column exists\n",
    "         ga_sessions_filled['geo_city_grouped'] = 'OTHER'\n",
    "\n",
    "    # print(\"-> Предобработка данных сессий завершена.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Предобработка и агрегация данных хитов (ga_hits_aggregated)\n",
    "    # --------------------------------------------------------------------------\n",
    "    # print(\"\\n-> Предобработка и агрегация данных хитов...\", file=sys.stderr)\n",
    "    ga_hits_filled = fill_hits_and_events(ga_hits_filled)\n",
    "\n",
    "    if 'hit_time' in ga_hits_filled.columns:\n",
    "        ga_hits_filled['hit_time_2'] = ga_hits_filled['hit_time'].notna().astype(int)\n",
    "\n",
    "    if 'hit_referer' in ga_hits_filled.columns: ga_hits_filled = encode_column_with_nulls(ga_hits_filled, 'hit_referer')\n",
    "\n",
    "    if 'event_category' in ga_hits_filled.columns and not ga_hits_filled.empty:\n",
    "        threshold = len(ga_hits_filled) * 0.01\n",
    "        value_counts = ga_hits_filled['event_category'].value_counts()\n",
    "        replace_dict = {category: 'OTHER' for category in value_counts[value_counts < threshold].index}\n",
    "        ga_hits_filled['event_category_grouped'] = ga_hits_filled['event_category'].replace(replace_dict)\n",
    "    elif 'event_category' in ga_hits_filled.columns: # Handle empty df but column exists\n",
    "        ga_hits_filled['event_category_grouped'] = 'OTHER'\n",
    "\n",
    "    if 'event_action' in ga_hits_filled.columns and not ga_hits_filled.empty:\n",
    "        threshold = len(ga_hits_filled) * 0.01\n",
    "        value_counts = ga_hits_filled['event_action'].value_counts()\n",
    "        replace_dict = {category: 'OTHER' for category in value_counts[value_counts < threshold].index}\n",
    "        ga_hits_filled['event_action_grouped'] = ga_hits_filled['event_action'].replace(replace_dict)\n",
    "    elif 'event_action' in ga_hits_filled.columns: # Handle empty df but column exists\n",
    "         ga_hits_filled['event_action_grouped'] = 'OTHER'\n",
    "\n",
    "    if 'event_label' in ga_hits_filled.columns: ga_hits_filled = encode_column_with_nulls(ga_hits_filled, 'event_label')\n",
    "\n",
    "    columns_to_drop_from_hits_filled = ['event_value','hit_date','hit_type', 'hit_time','hit_referer','event_label']\n",
    "    existing_cols_to_drop_hits = [col for col in columns_to_drop_from_hits_filled if col in ga_hits_filled.columns]\n",
    "    if existing_cols_to_drop_hits:\n",
    "        ga_hits_filled = ga_hits_filled.drop(columns=existing_cols_to_drop_hits, axis = 1)\n",
    "\n",
    "    columns_to_drop_from_sessions_filled = ['visit_date', 'visit_time', 'utm_source','utm_campaign','utm_adcontent','utm_keyword','device_screen_resolution','device_os','geo_city', 'year']\n",
    "    existing_cols_to_drop_sessions = [col for col in columns_to_drop_from_sessions_filled if col in ga_sessions_filled.columns]\n",
    "    if existing_cols_to_drop_sessions:\n",
    "        ga_sessions_filled = ga_sessions_filled.drop(columns=existing_cols_to_drop_sessions, axis = 1)\n",
    "\n",
    "\n",
    "    ga_hits_aggregated = aggregate_session_data(ga_hits_filled)\n",
    "    del ga_hits_filled; gc.collect()\n",
    "    if ga_hits_aggregated.empty:\n",
    "         print(\"Предупреждение: ga_hits_aggregated пуст после агрегации.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "    columns_to_drop_from_hits_aggregated = ['main_category','main_action']\n",
    "    existing_cols_to_drop_hits_agg = [col for col in columns_to_drop_from_hits_aggregated if col in ga_hits_aggregated.columns]\n",
    "    if existing_cols_to_drop_hits_agg:\n",
    "        ga_hits_aggregated = ga_hits_aggregated.drop(columns=existing_cols_to_drop_hits_agg, axis = 1)\n",
    "\n",
    "    # print(\"-> Предобработка и агрегация данных хитов завершена.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Объединение датафреймов\n",
    "    # --------------------------------------------------------------------------\n",
    "    # print(\"\\n-> Объединение датафреймов...\", file=sys.stderr)\n",
    "    # Не сохраняем промежуточные файлы merged_data.pkl / merged_data_2.pkl в функции предобработки\n",
    "    # Это опционально и должно быть вне этой функции, если нужно для отладки или хранения\n",
    "    data = merge_and_save(ga_sessions_filled, ga_hits_aggregated, output_pkl=None) # Убрал сохранение\n",
    "    del ga_sessions_filled, ga_hits_aggregated; gc.collect()\n",
    "\n",
    "    if data.empty:\n",
    "         print(\"Ошибка: Объединенный датафрейм пуст после слияния.\", file=sys.stderr)\n",
    "         return pd.DataFrame()\n",
    "\n",
    "\n",
    "    columns_to_drop_from_data = ['device_model','device_browser','device_model_encoded']\n",
    "    existing_cols_to_drop_data = [col for col in columns_to_drop_from_data if col in data.columns]\n",
    "    if existing_cols_to_drop_data:\n",
    "        data = data.drop(columns=existing_cols_to_drop_data, axis = 1)\n",
    "\n",
    "\n",
    "    # Удаляем действия без пользователей (строки с NaN в 'first_hit_number')\n",
    "    if 'first_hit_number' in data.columns:\n",
    "         data.dropna(subset=['first_hit_number'], inplace=True)\n",
    "    # else:\n",
    "         # print(\"'first_hit_number' столбец отсутствует.\", file=sys.stderr)\n",
    "\n",
    "    if data.empty:\n",
    "        print(\"Ошибка: Датафрейм пуст после финальной очистки (dropna).\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # print(\"\\n--- Функция preprocess_data завершена ---\", file=sys.stderr)\n",
    "    return data\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Основной исполняемый код: Замер времени предсказания\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"Запуск скрипта для замера времени предсказания.\", file=sys.stderr)\n",
    "    print(\"==============================================================================\", file=sys.stderr)\n",
    "\n",
    "    # Настройка путей к валидационным данным и обученной модели\n",
    "    file_path_hits_valid = 'ga_hits_valid.pkl' # Файл с валидационными хитами\n",
    "    file_path_sessions_valid = 'ga_sessions_valid.pkl' # Файл с валидационными сессиями\n",
    "    model_filepath = 'lgbm_model.pkl' # Файл с обученной моделью\n",
    "\n",
    "    # Убедимся, что файлы существуют\n",
    "    if not Path(file_path_hits_valid).exists():\n",
    "         print(f\"Ошибка: Валидационный файл хитов '{file_path_hits_valid}' не найден. Замер невозможен.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "    if not Path(file_path_sessions_valid).exists():\n",
    "         print(f\"Ошибка: Валидационный файл сессий '{file_path_sessions_valid}' не найден. Замер невозможен.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "    if not Path(model_filepath).exists():\n",
    "         print(f\"Ошибка: Файл обученной модели '{model_filepath}' не найден. Замер невозможен.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "\n",
    "    # --- Начало замера времени ---\n",
    "    start_time = time.time()\n",
    "    print(\"\\n--- Начало замера времени ---\", file=sys.stderr)\n",
    "\n",
    "    # 1) Прочитать датафреймы\n",
    "    print(\"Загрузка валидационных данных...\", file=sys.stderr)\n",
    "    sessions_valid_raw = None\n",
    "    hits_valid_raw = None\n",
    "    try:\n",
    "        sessions_valid_raw = pd.read_pickle(file_path_sessions_valid)\n",
    "        print(f\"Файл '{file_path_sessions_valid}' успешно загружен. Размер: {len(sessions_valid_raw)} строк.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке '{file_path_sessions_valid}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        hits_valid_raw = pd.read_pickle(file_path_hits_valid)\n",
    "        print(f\"Файл '{file_path_hits_valid}' успешно загружен. Размер: {len(hits_valid_raw)} строк.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке '{file_path_hits_valid}': {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if sessions_valid_raw.empty or hits_valid_raw.empty:\n",
    "         print(\"Ошибка: Загруженные валидационные датафреймы пусты. Замер невозможен.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "    # Применяем начальную предобработку 'not set'/'none'\n",
    "    print(\"Применение начальной предобработки 'not set'/'none' к валидационным данным...\", file=sys.stderr)\n",
    "    process_notset_none(sessions_valid_raw)\n",
    "    process_notset_none(hits_valid_raw)\n",
    "\n",
    "\n",
    "    # 2) Выполнить весь пиплайн предобработки\n",
    "    print(\"\\nВыполнение полного пайплайна предобработки на валидационных данных...\", file=sys.stderr)\n",
    "    data_valid_preprocessed = preprocess_data(sessions_valid_raw.copy(), hits_valid_raw.copy())\n",
    "\n",
    "    # Освобождаем память от исходных валидационных данных\n",
    "    del sessions_valid_raw, hits_valid_raw\n",
    "    gc.collect()\n",
    "\n",
    "    if data_valid_preprocessed.empty:\n",
    "         print(\"Ошибка: Данные после предобработки пусты. Предсказание невозможно.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "\n",
    "    # 3) Загрузить обученную модель\n",
    "    print(f\"\\nЗагрузка обученной модели из '{model_filepath}'...\", file=sys.stderr)\n",
    "    model_pipeline = None\n",
    "    try:\n",
    "        with open(model_filepath, 'rb') as f:\n",
    "            model_pipeline = pickle.load(f)\n",
    "        print(\"Модель успешно загружена.\", file=sys.stderr)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке модели: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    # Подготовка данных для предсказания (удаление целевого столбца, если есть, и session_id)\n",
    "    # preprocess_data не удаляет целевой столбец и session_id\n",
    "    print(\"Подготовка данных для предсказания...\", file=sys.stderr)\n",
    "    X_valid = data_valid_preprocessed.drop(columns=[target_column_name_for_prediction, 'session_id'], errors='ignore')\n",
    "\n",
    "    if X_valid.empty:\n",
    "         print(\"Ошибка: Нет признаков для предсказания после подготовки данных. Замер невозможен.\", file=sys.stderr)\n",
    "         sys.exit(1)\n",
    "\n",
    "    # 4) Выдача предикта\n",
    "    print(\"\\nВыполнение предсказания...\", file=sys.stderr)\n",
    "    predictions = None\n",
    "    try:\n",
    "        # Используйте .predict() для получения предсказанных классов\n",
    "        predictions = model_pipeline.predict(X_valid)\n",
    "        print(\"Предсказание выполнено.\", file=sys.stderr)\n",
    "\n",
    "        # Если нужны вероятности, используйте .predict_proba()\n",
    "        # probabilities = model_pipeline.predict_proba(X_valid)\n",
    "        # print(\"Предсказанные вероятности получены.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при выполнении предсказания: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Конец замера времени ---\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\"\\n--- Конец замера времени ---\", file=sys.stderr)\n",
    "    print(f\"Общее время выполнения пайплайна (загрузка валидации -> предобработка -> загрузка модели -> предсказание): {elapsed_time:.4f} секунд\", file=sys.stderr)\n",
    "\n",
    "\n",
    "    # Вывод первых нескольких предсказаний для проверки\n",
    "    print(\"\\nПервые 10 предсказанных значений:\", file=sys.stderr)\n",
    "    if predictions is not None:\n",
    "        print(predictions[:10], file=sys.stderr)\n",
    "    else:\n",
    "         print(\"Предсказания не были получены.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "    print(\"\\n==============================================================================\", file=sys.stderr)\n",
    "    print(\"Скрипт замера времени предсказания завершен.\", file=sys.stderr)\n",
    "    print(\"==============================================================================\", file=sys.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
